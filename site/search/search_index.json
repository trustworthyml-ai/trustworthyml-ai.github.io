{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Trustworthy Machine Learning","text":"<p>Welcome to the comprehensive resource hub for Trustworthy Machine Learning (TML). This site serves as a central repository for course materials, cutting-edge research, and community resources in the rapidly evolving field of trustworthy AI.</p>"},{"location":"#what-is-trustworthy-ml","title":"What is Trustworthy ML?","text":"<p>Trustworthy Machine Learning encompasses the principles and practices needed to build AI systems that are:</p> <ul> <li>Fair and unbiased across different populations</li> <li>Robust to adversarial attacks and distribution shifts  </li> <li>Transparent and interpretable in their decision-making</li> <li>Privacy-preserving in how they handle sensitive data</li> <li>Accountable for their predictions and recommendations</li> </ul>"},{"location":"#course-overview","title":"Course Overview","text":"<p>Our comprehensive course covers the fundamental concepts, state-of-the-art techniques, and practical implementations of trustworthy ML systems. Whether you're a student, researcher, or practitioner, you'll find valuable resources here.</p> <p>Fall 2024 Course</p> <p>The Trustworthy ML course is being offered in Fall 2024. Check out the syllabus for detailed information about topics, schedule, and assignments.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"StudentsResearchersPractitioners <ul> <li>Course Syllabus</li> <li>Schedule &amp; Lectures </li> <li>Assignments</li> <li>Project Guidelines</li> </ul> <ul> <li>Paper Library</li> <li>Key Topics</li> <li>Recent Advances</li> </ul> <ul> <li>Tools &amp; Frameworks</li> <li>Datasets</li> <li>Tutorials</li> </ul>"},{"location":"#featured-topics","title":"Featured Topics","text":"<ul> <li>Fairness in AI: Bias detection, mitigation strategies, and fairness metrics</li> <li>Adversarial Robustness: Defense mechanisms and robust training techniques</li> <li>Explainable AI: Interpretability methods and transparency tools</li> <li>Privacy-Preserving ML: Differential privacy, federated learning, and secure computation</li> <li>AI Safety: Alignment, safety verification, and risk assessment</li> </ul>"},{"location":"#stay-connected","title":"Stay Connected","text":"<p>Join our growing community of researchers, practitioners, and students working on trustworthy AI:</p> <ul> <li>\ud83d\udc19 GitHub Organization</li> <li>\ud83d\udc26 Twitter Updates</li> <li>\ud83d\udcac Community Discussions</li> <li>\ud83d\udcc5 Upcoming Events</li> </ul> <p>This resource is continuously updated with the latest research, tools, and community contributions. Contribute to help us build the most comprehensive TML resource on the web.</p>"},{"location":"blog/","title":"Trustworthy ML Blog","text":"<p>Welcome to our blog featuring insights, tutorials, and updates from the trustworthy ML community.</p> <p>Coming Soon</p> <p>The blog section is under development. We'll be featuring:</p> <ul> <li>Student project highlights</li> <li>Research paper summaries  </li> <li>Industry case studies</li> <li>Tool tutorials and reviews</li> <li>Community spotlights</li> </ul>"},{"location":"blog/#planned-content","title":"Planned Content","text":""},{"location":"blog/#research-spotlights","title":"Research Spotlights","text":"<p>Deep dives into important papers and their implications for practitioners.</p>"},{"location":"blog/#tool-reviews","title":"Tool Reviews","text":"<p>Hands-on reviews of new tools and frameworks in the trustworthy ML ecosystem.</p>"},{"location":"blog/#case-studies","title":"Case Studies","text":"<p>Real-world applications of trustworthy ML principles in industry.</p>"},{"location":"blog/#guest-posts","title":"Guest Posts","text":"<p>Contributions from researchers, practitioners, and students in the field.</p> <p>Stay tuned for regular updates and new content!</p>"},{"location":"community/contributing/","title":"Contributing to Trustworthy ML","text":"<p>We welcome contributions from the community! Whether you're a student, researcher, or practitioner, there are many ways to help build the most comprehensive resource for trustworthy machine learning.</p>"},{"location":"community/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"community/contributing/#content-contributions","title":"\ud83d\udcda Content Contributions","text":"<p>Research Papers - Submit new papers to our research library - Add summaries and practical insights to existing papers - Suggest paper categorizations and tags</p> <p>Tools &amp; Resources - Add new tools to our toolkit collection - Update installation instructions and compatibility info - Share usage examples and best practices</p> <p>Course Materials - Contribute assignment ideas and solutions - Share project examples and case studies - Suggest improvements to course structure</p>"},{"location":"community/contributing/#issue-reports","title":"\ud83d\udc1b Issue Reports","text":"<p>Found something wrong? Open an issue for:</p> <ul> <li>Broken links or outdated information</li> <li>Technical errors in code examples</li> <li>Missing important papers or tools</li> <li>Suggestions for new content sections</li> </ul>"},{"location":"community/contributing/#feature-requests","title":"\ud83d\udca1 Feature Requests","text":"<p>Help us improve the site by suggesting:</p> <ul> <li>New sections or content types</li> <li>Interactive tutorials or demos</li> <li>Community features (forums, events)</li> <li>Integration with external tools</li> </ul>"},{"location":"community/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":""},{"location":"community/contributing/#content-standards","title":"Content Standards","text":"<p>Quality Requirements - \u2705 Accurate and up-to-date information - \u2705 Clear, concise writing appropriate for academic audience - \u2705 Proper citations and attribution - \u2705 Reproducible code examples where applicable</p> <p>Paper Submissions - Must be peer-reviewed publications or high-quality preprints - Include full citation with venue and year - Add 2-3 sentence summary of significance - Tag with relevant topics using our taxonomy</p> <p>Tool Reviews - Test installation and basic functionality - Include clear installation instructions - Rate usability (\u2b50 to \u2b50\u2b50\u2b50\u2b50\u2b50) - Mention integration with other tools</p>"},{"location":"community/contributing/#style-guide","title":"Style Guide","text":"<p>Markdown Formatting <pre><code># Main Headings (H1)\n## Section Headings (H2)\n### Subsection Headings (H3)\n\n**Bold** for emphasis\n*Italics* for paper titles\n`code` for inline code\n</code></pre></p> <p>Paper References <pre><code>**[Paper Title](link)** (Authors, Year)  \n*Venue* | `tag1` `tag2` `tag3`  \nBrief description of contribution and significance.\n</code></pre></p> <p>Tool Listings <pre><code>**[Tool Name](link)**  \n*Organization* | Language | \u2b50\u2b50\u2b50\u2b50  \n```bash\npip install tool-name\n</code></pre> - Features: Key capabilities - Best for: Primary use cases - Integration: Compatible frameworks <pre><code>### Pull Request Process\n\n1. **Fork** the repository\n2. **Create** a feature branch (`git checkout -b feature/add-new-paper`)\n3. **Make** your changes following our style guide\n4. **Test** any code examples\n5. **Commit** with descriptive messages\n6. **Submit** a pull request with:\n   - Clear description of changes\n   - Link to any relevant issues\n   - Screenshots for UI changes\n\n### Review Process\n\nAll contributions go through peer review:\n\n1. **Automated checks**: Formatting, links, builds\n2. **Content review**: Accuracy, relevance, quality\n3. **Technical review**: Code functionality, reproducibility\n4. **Final approval**: Maintainer approval for merge\n\nTypical review time: 3-7 days for content, 1-3 days for fixes.\n\n## Recognition\n\nContributors are recognized in multiple ways:\n\n### Git Attribution\nAll contributions are tracked in git history with proper attribution.\n\n### Contributors Page\nRegular contributors are featured on our [contributors page](contributors.md) with:\n- Profile and bio\n- Areas of expertise\n- Contributions summary\n\n### Academic Citation\nFor significant contributions (new sections, major tool reviews):\n- Co-authorship consideration on related publications\n- Citation in academic papers referencing this resource\n\n## Community Guidelines\n\n### Code of Conduct\n\nWe are committed to providing a welcoming and inclusive environment:\n\n- \u2705 **Be respectful** in all interactions\n- \u2705 **Assume good intentions** from contributors\n- \u2705 **Provide constructive feedback** on contributions\n- \u2705 **Credit others' work** appropriately\n- \u274c **No harassment or discrimination** of any kind\n- \u274c **No spam or self-promotion** without value\n\n### Communication Channels\n\n- **GitHub Issues**: Bug reports, feature requests\n- **Discussions**: General questions, ideas, community chat\n- **Email**: [contact@trustworthyml.ai](mailto:contact@trustworthyml.ai)\n\n### Response Times\n\nWe aim to respond to:\n- **Issues**: Within 48 hours\n- **Pull requests**: Within 1 week\n- **Questions**: Within 24 hours\n\n## Getting Started\n\n### New Contributors\n\n1. **Read** the [README](https://github.com/trustworthyml-ai/trustworthyml-ai/blob/main/README.md)\n2. **Browse** [good first issues](https://github.com/trustworthyml-ai/trustworthyml-ai/labels/good%20first%20issue)\n3. **Join** our [discussion forum](https://github.com/trustworthyml-ai/trustworthyml-ai/discussions)\n4. **Introduce yourself** and ask questions!\n\n### Quick Contributions\n\nEasy ways to start contributing:\n\n- **Fix typos** or broken links\n- **Add missing citations** to paper references  \n- **Update installation instructions** for tools\n- **Add tags** to untagged content\n- **Share usage examples** for existing tools\n\n### Substantial Contributions\n\nFor larger contributions:\n\n1. **Open an issue** to discuss your idea first\n2. **Get feedback** from maintainers and community\n3. **Plan your approach** with milestones\n4. **Submit incremental PRs** for review\n\n## Technical Setup\n\n### Local Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/trustworthyml-ai/trustworthyml-ai.git\ncd trustworthyml-ai\n\n# Install MkDocs and dependencies\npip install mkdocs-material\npip install mkdocs-git-revision-date-localized-plugin\n\n# Serve locally\nmkdocs serve\n</code></pre></p> <p>Visit <code>http://localhost:8000</code> to preview your changes.</p>"},{"location":"community/contributing/#dependencies","title":"Dependencies","text":"<p>The site uses: - MkDocs: Static site generator - Material theme: Modern, responsive design - Git plugins: Automatic timestamps and contributors - GitHub Actions: Automated deployment</p>"},{"location":"community/contributing/#questions","title":"Questions?","text":"<p>Need help getting started? Check our FAQ or open a discussion.</p> <p>Found a bug or security issue? Please report it via GitHub Issues or email us directly for security concerns.</p> <p>Thank you for helping build the trustworthy ML community! \ud83d\ude80</p>"},{"location":"course/assignments/","title":"Assignments","text":"<p>Detailed information about course assignments, including specifications, submission guidelines, and grading rubrics.</p> <p>Under Development</p> <p>Assignment details will be posted here as they are released throughout the semester.</p>"},{"location":"course/assignments/#assignment-overview","title":"Assignment Overview","text":"Assignment Topic Weight Release Due A1 Bias Detection &amp; Mitigation 10% Week 4 Week 6 A2 Adversarial Robustness 10% Week 7 Week 9 A3 Model Interpretability 10% Week 10 Week 11 A4 Privacy-Preserving ML 10% Week 12 Week 14"},{"location":"course/assignments/#assignment-specifications","title":"Assignment Specifications","text":""},{"location":"course/assignments/#assignment-1-bias-detection-mitigation","title":"Assignment 1: Bias Detection &amp; Mitigation","text":"<p>Release: Week 4 | Due: Week 6</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-2-adversarial-robustness","title":"Assignment 2: Adversarial Robustness","text":"<p>Release: Week 7 | Due: Week 9</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-3-model-interpretability","title":"Assignment 3: Model Interpretability","text":"<p>Release: Week 10 | Due: Week 11</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-4-privacy-preserving-ml","title":"Assignment 4: Privacy-Preserving ML","text":"<p>Release: Week 12 | Due: Week 14</p> <p>Coming soon...</p>"},{"location":"course/assignments/#submission-guidelines","title":"Submission Guidelines","text":"<ul> <li>Submit via course management system</li> <li>Include all code, data, and documentation</li> <li>Follow academic integrity policies</li> <li>Late penalty: 10% per day</li> </ul> <p>This page will be updated as assignments are released.</p>"},{"location":"course/projects/","title":"Final Projects","text":"<p>The final project is a team-based research project where you'll explore an open problem in trustworthy machine learning.</p>"},{"location":"course/projects/#project-overview","title":"Project Overview","text":"<p>Team Size: 3-4 students Timeline: 8 weeks (Weeks 8-15) Weight: 30% of final grade  </p>"},{"location":"course/projects/#project-components","title":"Project Components","text":"Component Weight Due Date Proposal 5% Week 11 Progress Report 5% Week 13 Final Presentation 10% Week 15 Final Report 10% Finals Week"},{"location":"course/projects/#project-guidelines","title":"Project Guidelines","text":""},{"location":"course/projects/#scope-and-topics","title":"Scope and Topics","text":"<p>Your project should address a research question in one or more areas of trustworthy ML:</p> <ul> <li>Fairness &amp; Bias: Novel detection methods, mitigation algorithms, evaluation metrics</li> <li>Robustness: New attack techniques, defense mechanisms, certified methods  </li> <li>Interpretability: Explanation methods, evaluation frameworks, human studies</li> <li>Privacy: Differential privacy mechanisms, federated learning innovations</li> <li>Safety &amp; Alignment: Value learning, reward modeling, verification methods</li> </ul>"},{"location":"course/projects/#requirements","title":"Requirements","text":"<ul> <li>Novelty: Propose new methods or provide new insights</li> <li>Implementation: Working code with experiments</li> <li>Evaluation: Rigorous experimental validation</li> <li>Writing: Clear technical exposition</li> </ul>"},{"location":"course/projects/#deliverables","title":"Deliverables","text":""},{"location":"course/projects/#project-proposal-2-pages","title":"Project Proposal (2 pages)","text":"<p>Due: Week 11</p> <p>Required Sections: 1. Problem Statement: What challenge are you addressing? 2. Related Work: Brief survey of relevant papers (5-10 papers) 3. Approach: Your proposed method or analysis 4. Evaluation Plan: Datasets, metrics, baselines 5. Timeline: Milestones for remaining weeks</p>"},{"location":"course/projects/#progress-report-1-page","title":"Progress Report (1 page)","text":"<p>Due: Week 13</p> <p>Required Sections: 1. Progress Summary: What you've completed 2. Preliminary Results: Initial findings or implementation 3. Challenges: Issues encountered and solutions 4. Updated Timeline: Revised plan for final weeks</p>"},{"location":"course/projects/#final-presentation-15-minutes","title":"Final Presentation (15 minutes)","text":"<p>Date: Week 15</p> <p>Presentation Structure: - Problem motivation (2-3 min) - Technical approach (5-6 min) - Experimental results (4-5 min) - Conclusions and future work (2-3 min) - Q&amp;A (5 min)</p>"},{"location":"course/projects/#final-report-8-10-pages","title":"Final Report (8-10 pages)","text":"<p>Due: Finals Week</p> <p>Required Sections: 1. Abstract: Problem, approach, key findings 2. Introduction: Motivation and problem statement 3. Related Work: Comprehensive literature review 4. Method: Detailed technical description 5. Experiments: Setup, results, analysis 6. Discussion: Limitations, implications, future work 7. Conclusion: Summary of contributions</p>"},{"location":"course/projects/#project-ideas","title":"Project Ideas","text":""},{"location":"course/projects/#fairness-projects","title":"Fairness Projects","text":"<ul> <li>Intersectional Fairness: Methods for multi-attribute fairness</li> <li>Dynamic Fairness: Fairness that adapts over time</li> <li>Fairness in NLP: Bias detection in language models</li> <li>Causal Fairness: Using causal inference for fair decisions</li> </ul>"},{"location":"course/projects/#robustness-projects","title":"Robustness Projects","text":"<ul> <li>Universal Adversarial Perturbations: Domain-specific attacks</li> <li>Certified Defense: Improving scalability of verification</li> <li>Real-World Robustness: Robustness to natural distribution shifts</li> <li>Robust Federated Learning: Security in distributed training</li> </ul>"},{"location":"course/projects/#interpretability-projects","title":"Interpretability Projects","text":"<ul> <li>Counterfactual Explanations: Generating actionable explanations  </li> <li>Explanation Evaluation: New metrics for explanation quality</li> <li>Interactive Explanations: Human-in-the-loop explanation systems</li> <li>Interpretable Deep Learning: Inherently interpretable architectures</li> </ul>"},{"location":"course/projects/#privacy-projects","title":"Privacy Projects","text":"<ul> <li>Local Differential Privacy: Privacy without trusted curator</li> <li>Private Representation Learning: Privacy-preserving embeddings</li> <li>Membership Inference Defense: Protecting against privacy attacks</li> <li>Federated Learning Privacy: Novel privacy-utility trade-offs</li> </ul>"},{"location":"course/projects/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"course/projects/#technical-quality-40","title":"Technical Quality (40%)","text":"<ul> <li>Correctness: Sound methodology and implementation</li> <li>Novelty: Original insights or approaches  </li> <li>Rigor: Thorough experimental validation</li> <li>Reproducibility: Clear implementation details</li> </ul>"},{"location":"course/projects/#presentation-30","title":"Presentation (30%)","text":"<ul> <li>Clarity: Clear problem statement and solution</li> <li>Organization: Logical flow and structure</li> <li>Delivery: Effective oral presentation skills</li> <li>Visual Design: Quality figures and slides</li> </ul>"},{"location":"course/projects/#writing-30","title":"Writing (30%)","text":"<ul> <li>Technical Writing: Clear, precise exposition</li> <li>Related Work: Comprehensive literature coverage</li> <li>Analysis: Thoughtful discussion of results</li> <li>Formatting: Professional presentation</li> </ul>"},{"location":"course/projects/#resources","title":"Resources","text":""},{"location":"course/projects/#datasets","title":"Datasets","text":"<ul> <li>Fairness: Adult, COMPAS, CelebA, Folktables</li> <li>Robustness: CIFAR-10/100, ImageNet, MNIST</li> <li>Privacy: See federated learning benchmarks</li> <li>General: Papers with Datasets collections</li> </ul>"},{"location":"course/projects/#computing-resources","title":"Computing Resources","text":"<ul> <li>Google Colab Pro: For small-scale experiments</li> <li>Course Cluster: For larger computational needs</li> <li>Cloud Credits: Limited AWS/Azure credits available</li> </ul>"},{"location":"course/projects/#collaboration-tools","title":"Collaboration Tools","text":"<ul> <li>GitHub: Version control and collaboration</li> <li>Overleaf: Collaborative LaTeX writing</li> <li>Slack: Team communication</li> <li>Office Hours: Weekly project consultations</li> </ul>"},{"location":"course/projects/#timeline","title":"Timeline","text":""},{"location":"course/projects/#week-8-10-team-formation-topic-selection","title":"Week 8-10: Team Formation &amp; Topic Selection","text":"<ul> <li>Form teams and explore project ideas</li> <li>Read relevant papers and identify gaps</li> <li>Discuss ideas during office hours</li> </ul>"},{"location":"course/projects/#week-11-proposal-submission","title":"Week 11: Proposal Submission","text":"<ul> <li>Submit 2-page project proposal</li> <li>Receive feedback from instructors</li> </ul>"},{"location":"course/projects/#week-12-13-implementation-experiments","title":"Week 12-13: Implementation &amp; Experiments","text":"<ul> <li>Implement core methodology</li> <li>Run initial experiments</li> <li>Submit progress report</li> </ul>"},{"location":"course/projects/#week-14-final-push","title":"Week 14: Final Push","text":"<ul> <li>Complete experiments and analysis</li> <li>Prepare presentation slides</li> <li>Draft final report</li> </ul>"},{"location":"course/projects/#week-15-presentations","title":"Week 15: Presentations","text":"<ul> <li>Present findings to class</li> <li>Provide peer feedback</li> <li>Finalize written report</li> </ul>"},{"location":"course/projects/#past-project-examples","title":"Past Project Examples","text":""},{"location":"course/projects/#successful-projects-previous-years","title":"Successful Projects (Previous Years)","text":"<ul> <li>\"Fairness-Aware Multi-Task Learning\" - Novel algorithm with theoretical analysis</li> <li>\"Robust Vision Transformers\" - Comprehensive robustness evaluation</li> <li>\"Explaining Neural Recommendation Systems\" - Human evaluation study</li> <li>\"Privacy-Preserving Graph Neural Networks\" - DP mechanisms for graph data</li> </ul> <p>For questions about projects, contact the teaching team during office hours or via email.</p>"},{"location":"course/schedule/","title":"Course Schedule","text":"<p>Fall 2024 | Trustworthy Machine Learning</p> <p>Schedule Updates</p> <p>This schedule is subject to change. Check regularly for updates and assignment due dates.</p>"},{"location":"course/schedule/#week-by-week-schedule","title":"Week-by-Week Schedule","text":""},{"location":"course/schedule/#module-1-foundations","title":"Module 1: Foundations","text":"<p>Week 1 (Aug 28-30): Introduction to Trustworthy ML - Tuesday: Course overview, motivation, and key challenges - Thursday: Case studies of ML failures and societal impact - Readings: Barocas et al. Ch. 1, recent news articles on AI bias - Due: Background survey (not graded)</p> <p>Week 2 (Sep 4-6): Ethics and Fairness Foundations - Tuesday: Philosophical foundations of fairness - Thursday: Legal and regulatory landscape (GDPR, algorithmic auditing) - Readings: Barocas et al. Ch. 2, EU AI Act summary - Guest: Industry expert on AI governance</p> <p>Week 3 (Sep 11-13): Mathematical Frameworks - Tuesday: Formalizing fairness: metrics and trade-offs - Thursday: Statistical parity vs. individual fairness - Readings: Dwork et al. 2012, Hardt et al. 2016 - Lab: Setting up fairness toolkits (AIF360, Fairlearn)</p>"},{"location":"course/schedule/#module-2-fairness-bias","title":"Module 2: Fairness &amp; Bias","text":"<p>Week 4 (Sep 18-20): Bias in ML Pipelines - Tuesday: Sources of bias: data, algorithms, deployment - Thursday: Measurement and detection techniques - Readings: Mehrabi et al. 2021 (bias survey) - Assignment 1 Released: Bias Detection and Mitigation</p> <p>Week 5 (Sep 25-27): Fairness Interventions - Tuesday: Pre-processing: data cleaning and augmentation - Thursday: In-processing: fair learning algorithms - Readings: Kamiran &amp; Calders 2012, Zafar et al. 2017 - Lab: Implementing bias mitigation techniques</p> <p>Week 6 (Oct 2-4): Advanced Fairness Topics - Tuesday: Post-processing and calibration - Thursday: Intersectionality and multi-attribute fairness - Readings: Pleiss et al. 2017, Foulds &amp; Pan 2020 - Assignment 1 Due: End of week</p>"},{"location":"course/schedule/#module-3-robustness-security","title":"Module 3: Robustness &amp; Security","text":"<p>Week 7 (Oct 9-11): Adversarial Examples - Tuesday: Discovery and taxonomy of adversarial attacks - Thursday: White-box vs. black-box attacks - Readings: Szegedy et al. 2013, Goodfellow et al. 2014 - Midterm Review: Friday study session - Assignment 2 Released: Adversarial Robustness</p> <p>Week 8 (Oct 16-18): Adversarial Defenses - Tuesday: MIDTERM EXAM (Modules 1-3) - Thursday: Adversarial training and certified defenses - Readings: Madry et al. 2017, Cohen et al. 2019 - Lab: Implementing PGD attacks and defenses</p> <p>Week 9 (Oct 23-25): Robustness Beyond Adversaries - Tuesday: Distribution shift and domain adaptation - Thursday: Robust optimization and uncertainty quantification - Readings: Koh &amp; Liang 2017, Ovadia et al. 2019 - Assignment 2 Due: End of week</p>"},{"location":"course/schedule/#module-4-interpretability-explainability","title":"Module 4: Interpretability &amp; Explainability","text":"<p>Week 10 (Oct 30-Nov 1): Interpretation Methods - Tuesday: Global vs. local interpretability - Thursday: Feature attribution: SHAP, LIME, and integrated gradients - Readings: Ribeiro et al. 2016, Lundberg &amp; Lee 2017 - Assignment 3 Released: Model Interpretability</p> <p>Week 11 (Nov 6-8): Evaluation and Human Factors - Tuesday: Evaluating explanation quality - Thursday: Human-AI interaction and explanation effectiveness - Readings: Adebayo et al. 2018, Poursabzi-Sangdeh et al. 2021 - Project Proposals Due: November 8 - Assignment 3 Due: End of week</p>"},{"location":"course/schedule/#module-5-privacy-preserving-ml","title":"Module 5: Privacy-Preserving ML","text":"<p>Week 12 (Nov 13-15): Differential Privacy - Tuesday: DP fundamentals and mechanisms - Thursday: Private SGD and deep learning applications - Readings: Dwork &amp; Roth 2014, Abadi et al. 2016 - Assignment 4 Released: Privacy-Preserving Techniques</p> <p>Week 13 (Nov 20-22): Federated and Secure Learning - Tuesday: Federated learning: algorithms and challenges - Thursday: NO CLASS - Thanksgiving Break - Readings: McMahan et al. 2017, Li et al. 2020 - Project Progress Reports Due: November 22</p>"},{"location":"course/schedule/#module-6-ai-safety-alignment","title":"Module 6: AI Safety &amp; Alignment","text":"<p>Week 14 (Nov 27-29): AI Safety Fundamentals - Tuesday: The alignment problem and value learning - Thursday: Reward modeling and RLHF - Readings: Russell 2019 Ch. 8, Christiano et al. 2017 - Assignment 4 Due: End of week</p> <p>Week 15 (Dec 4-6): Deployment and Monitoring - Tuesday: MLOps for trustworthy systems - Thursday: Continuous monitoring and auditing - Readings: Sculley et al. 2015, Breck et al. 2019 - Final Project Presentations: December 6</p>"},{"location":"course/schedule/#finals-week-dec-11-15","title":"Finals Week (Dec 11-15)","text":"<ul> <li>Final Project Reports Due: December 13, 11:59 PM</li> </ul>"},{"location":"course/schedule/#assignment-schedule","title":"Assignment Schedule","text":"Assignment Topic Released Due Weight Assignment 1 Bias Detection &amp; Mitigation Week 4 Week 6 10% Assignment 2 Adversarial Robustness Week 7 Week 9 10% Assignment 3 Model Interpretability Week 10 Week 11 10% Assignment 4 Privacy-Preserving ML Week 12 Week 14 10% Midterm Modules 1-3 - Week 8 20% Final Project Research Project Week 8 Finals 30% Participation Various - - 10%"},{"location":"course/schedule/#project-timeline","title":"Project Timeline","text":""},{"location":"course/schedule/#key-milestones","title":"Key Milestones","text":"<ul> <li>Week 8: Project topics announced</li> <li>Week 11: Project proposals due (2 pages)</li> <li>Week 13: Progress reports due (1 page)</li> <li>Week 15: Final presentations (15 min each)</li> <li>Finals: Final reports due (8-10 pages)</li> </ul>"},{"location":"course/schedule/#suggested-project-areas","title":"Suggested Project Areas","text":"<ol> <li>Fairness: Novel bias detection or mitigation methods</li> <li>Robustness: New attack methods or defense techniques  </li> <li>Interpretability: Improved explanation methods or evaluation</li> <li>Privacy: Advanced DP mechanisms or federated learning</li> <li>Safety: AI alignment or verification approaches</li> <li>Applications: Domain-specific trustworthy ML solutions</li> </ol>"},{"location":"course/schedule/#reading-materials","title":"Reading Materials","text":""},{"location":"course/schedule/#required-textbooks-free-online","title":"Required Textbooks (Free Online)","text":"<ul> <li>Fairness and Machine Learning - Barocas, Hardt, Narayanan</li> <li>Interpretable Machine Learning - Molnar</li> </ul>"},{"location":"course/schedule/#supplementary-resources","title":"Supplementary Resources","text":"<ul> <li>Course slides and lecture notes</li> <li>Research papers (linked in weekly readings)</li> <li>Tutorial videos and demos</li> <li>Tool documentation and examples</li> </ul>"},{"location":"course/schedule/#office-hours","title":"Office Hours","text":"<ul> <li>Instructor: Tuesdays 2-4 PM, Fridays 1-2 PM</li> <li>TAs: </li> <li>TA1: Mondays 3-5 PM, Wednesdays 10-12 PM</li> <li>TA2: Thursdays 2-4 PM, Saturdays 10-12 PM</li> </ul>"},{"location":"course/schedule/#important-policies","title":"Important Policies","text":""},{"location":"course/schedule/#attendance","title":"Attendance","text":"<ul> <li>Attendance is strongly encouraged but not mandatory</li> <li>Participation grade includes in-class discussion and online forums</li> <li>Notify instructor in advance for extended absences</li> </ul>"},{"location":"course/schedule/#late-policy","title":"Late Policy","text":"<ul> <li>10% penalty per day late for assignments</li> <li>Extensions granted for documented emergencies</li> <li>Final project has hard deadline (no extensions)</li> </ul>"},{"location":"course/schedule/#collaboration","title":"Collaboration","text":"<ul> <li>Individual assignments must be completed independently</li> <li>Group discussion encouraged, but write-ups must be original</li> <li>Final projects are team-based (3-4 students)</li> <li>AI tools (ChatGPT, Copilot) usage must be disclosed</li> </ul> <p>Schedule last updated: {{ git_revision_date_localized }}</p>"},{"location":"course/syllabus/","title":"Course Syllabus: Trustworthy Machine Learning","text":""},{"location":"course/syllabus/#course-information","title":"Course Information","text":"<p>Course Title: Trustworthy Machine Learning Semester: Fall 2024 Prerequisites: Machine Learning fundamentals, Linear Algebra, Statistics, Python programming Credits: 3  </p>"},{"location":"course/syllabus/#course-description","title":"Course Description","text":"<p>This course provides a comprehensive introduction to the principles, methods, and applications of trustworthy machine learning. Students will learn to design, implement, and evaluate ML systems that are fair, robust, transparent, privacy-preserving, and accountable. The course combines theoretical foundations with hands-on projects using real-world datasets and modern tools.</p>"},{"location":"course/syllabus/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Understand Core Concepts: Define and explain the key principles of trustworthy ML including fairness, robustness, interpretability, privacy, and accountability</li> <li>Identify Vulnerabilities: Recognize potential sources of bias, adversarial attacks, and privacy leaks in ML systems</li> <li>Apply Mitigation Techniques: Implement state-of-the-art methods for bias mitigation, adversarial defense, and privacy protection</li> <li>Evaluate Systems: Use appropriate metrics and evaluation frameworks to assess the trustworthiness of ML models</li> <li>Design Solutions: Architect end-to-end trustworthy ML systems for real-world applications</li> </ol>"},{"location":"course/syllabus/#course-topics","title":"Course Topics","text":""},{"location":"course/syllabus/#module-1-foundations-weeks-1-3","title":"Module 1: Foundations (Weeks 1-3)","text":"<ul> <li>Introduction to Trustworthy ML</li> <li>Ethics in AI and ML</li> <li>Legal and regulatory landscape</li> <li>Case studies of ML failures</li> </ul>"},{"location":"course/syllabus/#module-2-fairness-bias-weeks-4-6","title":"Module 2: Fairness &amp; Bias (Weeks 4-6)","text":"<ul> <li>Types of bias in ML systems</li> <li>Fairness definitions and metrics</li> <li>Pre-processing, in-processing, and post-processing techniques</li> <li>Intersectionality and group fairness</li> </ul>"},{"location":"course/syllabus/#module-3-robustness-security-weeks-7-9","title":"Module 3: Robustness &amp; Security (Weeks 7-9)","text":"<ul> <li>Adversarial examples and attacks</li> <li>Certified defenses and robust training</li> <li>Distribution shift and domain adaptation</li> <li>Model stealing and membership inference attacks</li> </ul>"},{"location":"course/syllabus/#module-4-interpretability-explainability-weeks-10-11","title":"Module 4: Interpretability &amp; Explainability (Weeks 10-11)","text":"<ul> <li>Global vs. local interpretability</li> <li>Feature importance and attribution methods</li> <li>Counterfactual explanations</li> <li>Human-AI interaction and explanation quality</li> </ul>"},{"location":"course/syllabus/#module-5-privacy-preserving-ml-weeks-12-13","title":"Module 5: Privacy-Preserving ML (Weeks 12-13)","text":"<ul> <li>Differential privacy fundamentals</li> <li>Federated learning</li> <li>Secure multi-party computation</li> <li>Privacy attacks and defenses</li> </ul>"},{"location":"course/syllabus/#module-6-ai-safety-alignment-weeks-14-15","title":"Module 6: AI Safety &amp; Alignment (Weeks 14-15)","text":"<ul> <li>AI alignment problem</li> <li>Value learning and reward modeling</li> <li>Safety verification and testing</li> <li>Deployment considerations and monitoring</li> </ul>"},{"location":"course/syllabus/#assessment","title":"Assessment","text":"Component Weight Description Assignments (4) 40% Individual coding and written assignments Midterm Exam 20% In-class examination on Modules 1-3 Final Project 30% Team-based research project with presentation Participation 10% Class discussion, paper reviews, peer feedback"},{"location":"course/syllabus/#assignment-schedule","title":"Assignment Schedule","text":"<ul> <li>Assignment 1: Bias Detection and Mitigation (Week 5)</li> <li>Assignment 2: Adversarial Robustness (Week 8)  </li> <li>Assignment 3: Model Interpretability (Week 11)</li> <li>Assignment 4: Privacy-Preserving Techniques (Week 13)</li> </ul>"},{"location":"course/syllabus/#final-project","title":"Final Project","text":"<p>Teams of 3-4 students will work on an original research project addressing a trustworthy ML challenge. Projects must include: - Literature review and problem formulation - Technical approach and implementation - Experimental evaluation - Written report (8-10 pages) - Final presentation (15 minutes)</p>"},{"location":"course/syllabus/#required-resources","title":"Required Resources","text":""},{"location":"course/syllabus/#textbooks","title":"Textbooks","text":"<ul> <li>Fairness and Machine Learning by Barocas, Hardt, and Narayanan (free online)</li> <li>Interpretable Machine Learning by Christoph Molnar (free online)</li> </ul>"},{"location":"course/syllabus/#software-tools","title":"Software &amp; Tools","text":"<ul> <li>Python 3.8+ with scikit-learn, PyTorch/TensorFlow</li> <li>Fairness toolkits: AIF360, Fairlearn</li> <li>Privacy libraries: Opacus, PySyft</li> <li>Interpretability tools: SHAP, LIME, Captum</li> </ul>"},{"location":"course/syllabus/#computing-resources","title":"Computing Resources","text":"<ul> <li>Google Colab Pro or similar cloud platform</li> <li>Course cluster access for larger experiments</li> </ul>"},{"location":"course/syllabus/#policies","title":"Policies","text":""},{"location":"course/syllabus/#late-policy","title":"Late Policy","text":"<ul> <li>10% penalty per day late</li> <li>Extensions granted for documented emergencies</li> <li>No late submissions accepted for final project</li> </ul>"},{"location":"course/syllabus/#academic-integrity","title":"Academic Integrity","text":"<p>All work must be original. Collaboration is encouraged on projects but must be clearly documented. Use of AI tools (ChatGPT, Copilot) must be disclosed and appropriately credited.</p>"},{"location":"course/syllabus/#accessibility","title":"Accessibility","text":"<p>Students with documented disabilities should contact the Office of Disability Services to arrange reasonable accommodations.</p>"},{"location":"course/syllabus/#important-dates","title":"Important Dates","text":"<ul> <li>Week 3: Assignment 1 released</li> <li>Week 6: Midterm exam</li> <li>Week 8: Project proposal due</li> <li>Week 12: Project progress report</li> <li>Week 15: Final presentations</li> <li>Finals Week: Final project reports due</li> </ul> <p>This syllabus is subject to change with advance notice. Check the course website regularly for updates.</p>"},{"location":"research/papers/","title":"Research Paper Library","text":"<p>A curated collection of seminal and recent papers in trustworthy machine learning. Papers are organized by topic and include our commentary on significance and practical implications.</p> <p>Search &amp; Filter</p> <p>Use <code>Ctrl+F</code> to search for specific topics, authors, or venues. Papers are tagged with key concepts for easy discovery.</p>"},{"location":"research/papers/#foundational-papers","title":"Foundational Papers","text":""},{"location":"research/papers/#fairness-bias","title":"Fairness &amp; Bias","text":"<p>Seminal Works</p> <ul> <li> <p>Fairness Through Awareness (Dwork et al., 2012) ITCS 2012 | <code>individual-fairness</code> <code>awareness</code>   Introduces the concept of individual fairness and awareness in algorithmic decision-making.</p> </li> <li> <p>Equality of Opportunity in Supervised Learning (Hardt et al., 2016) NIPS 2016 | <code>group-fairness</code> <code>equalized-odds</code>   Defines equalized odds and equality of opportunity for binary classification.</p> </li> <li> <p>Fairness Definitions Explained (Verma &amp; Rubin, 2018) IEEE FATES 2018 | <code>survey</code> <code>fairness-metrics</code>   Comprehensive survey of 20+ fairness definitions with mathematical formulations.</p> </li> </ul> <p>Recent Advances</p> <ul> <li> <p>Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned (Bellamy et al., 2019) WSDM 2019 | <code>aif360</code> <code>toolkit</code> <code>industry</code>   Practical insights from deploying fairness-aware ML in enterprise settings.</p> </li> <li> <p>Intersectional Fairness: A Fractal Approach (Foulds et al., 2020) FAccT 2020 | <code>intersectionality</code> <code>subgroup-fairness</code>   Novel approach to handling fairness across intersecting protected attributes.</p> </li> </ul>"},{"location":"research/papers/#robustness-adversarial-ml","title":"Robustness &amp; Adversarial ML","text":"<p>Foundational</p> <ul> <li> <p>Intriguing Properties of Neural Networks (Szegedy et al., 2013) ICLR 2014 | <code>adversarial-examples</code> <code>discovery</code>   First systematic study of adversarial examples in deep neural networks.</p> </li> <li> <p>Explaining and Harnessing Adversarial Examples (Goodfellow et al., 2014) ICLR 2015 | <code>fgsm</code> <code>linear-hypothesis</code>   Introduces FGSM attack and linear hypothesis for adversarial vulnerability.</p> </li> <li> <p>Towards Deep Learning Models Resistant to Adversarial Attacks (Madry et al., 2017) ICLR 2018 | <code>pgd</code> <code>adversarial-training</code>   Establishes PGD as the gold standard for adversarial training evaluation.</p> </li> </ul> <p>Certified Defenses</p> <ul> <li> <p>Certified Adversarial Robustness via Randomized Smoothing (Cohen et al., 2019) ICML 2019 | <code>certified-defense</code> <code>randomized-smoothing</code>   Scalable approach to obtaining robustness certificates using Gaussian noise.</p> </li> <li> <p>Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers (Salman et al., 2019) NeurIPS 2019 | <code>certified-training</code> <code>smoothing</code>   Combines adversarial training with randomized smoothing for stronger guarantees.</p> </li> </ul>"},{"location":"research/papers/#interpretability-explainability","title":"Interpretability &amp; Explainability","text":"<p>Core Methods</p> <ul> <li> <p>\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier (Ribeiro et al., 2016) KDD 2016 | <code>lime</code> <code>local-explanations</code>   Introduces LIME for locally interpretable model-agnostic explanations.</p> </li> <li> <p>A Unified Approach to Interpreting Model Predictions (Lundberg &amp; Lee, 2017) NIPS 2017 | <code>shap</code> <code>shapley-values</code>   SHAP: Unified framework based on cooperative game theory.</p> </li> <li> <p>Attention is All You Need (Vaswani et al., 2017) NIPS 2017 | <code>attention</code> <code>transformers</code> <code>interpretability</code>   While primarily an architecture paper, attention mechanisms provide built-in interpretability.</p> </li> </ul> <p>Evaluation &amp; Benchmarking</p> <ul> <li> <p>Evaluating the Visualization of What a Deep Neural Network Has Learned (Simonyan et al., 2013) ICLR 2014 | <code>saliency-maps</code> <code>evaluation</code>   Early work on evaluating explanation quality through perturbation analysis.</p> </li> <li> <p>Sanity Checks for Saliency Maps (Adebayo et al., 2018) NeurIPS 2018 | <code>sanity-checks</code> <code>saliency-evaluation</code>   Demonstrates that many explanation methods fail basic sanity checks.</p> </li> </ul>"},{"location":"research/papers/#privacy-preserving-ml","title":"Privacy-Preserving ML","text":"<p>Differential Privacy</p> <ul> <li> <p>Deep Learning with Differential Privacy (Abadi et al., 2016) CCS 2016 | <code>differential-privacy</code> <code>sgd</code> <code>deep-learning</code>   First practical application of differential privacy to deep learning training.</p> </li> <li> <p>The Algorithmic Foundations of Differential Privacy (Dwork &amp; Roth, 2014) Foundations and Trends | <code>dp-foundations</code> <code>survey</code>   Comprehensive theoretical foundation of differential privacy.</p> </li> </ul> <p>Federated Learning</p> <ul> <li> <p>Communication-Efficient Learning of Deep Networks from Decentralized Data (McMahan et al., 2017) AISTATS 2017 | <code>federated-learning</code> <code>fedavg</code>   Introduces federated learning and the FedAvg algorithm.</p> </li> <li> <p>Towards Federated Learning at Scale: System Design (Bonawitz et al., 2019) MLSys 2019 | <code>federated-systems</code> <code>scale</code>   System design considerations for large-scale federated learning deployment.</p> </li> </ul>"},{"location":"research/papers/#recent-research-2023-2024","title":"Recent Research (2023-2024)","text":""},{"location":"research/papers/#emerging-topics","title":"Emerging Topics","text":"<ul> <li> <p>Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022) Anthropic | <code>constitutional-ai</code> <code>alignment</code> <code>safety</code>   Novel approach to AI alignment using constitutional principles and AI feedback.</p> </li> <li> <p>Red Teaming Language Models with Language Models (Perez et al., 2022) EMNLP 2022 | <code>red-teaming</code> <code>llm-safety</code> <code>automated-testing</code>   Automated red teaming approach for identifying harmful LLM behaviors.</p> </li> <li> <p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (Bai et al., 2022) Anthropic | <code>rlhf</code> <code>helpfulness</code> <code>harmlessness</code>   Balancing helpfulness and harmlessness in conversational AI systems.</p> </li> </ul>"},{"location":"research/papers/#benchmark-papers","title":"Benchmark Papers","text":"<ul> <li> <p>BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation (Dhamala et al., 2021) FAccT 2021 | <code>bias-benchmarks</code> <code>language-generation</code>   Comprehensive benchmark for measuring bias in text generation models.</p> </li> <li> <p>RobustBench: a Standardized Adversarial Robustness Benchmark (Croce et al., 2020) NeurIPS 2021 | <code>robustness-benchmark</code> <code>evaluation</code>   Standardized benchmark and leaderboard for adversarial robustness evaluation.</p> </li> </ul>"},{"location":"research/papers/#paper-collections-by-venue","title":"Paper Collections by Venue","text":""},{"location":"research/papers/#top-tier-conferences","title":"Top-Tier Conferences","text":"FAccT (Fairness, Accountability, Transparency)ICML/NeurIPSICLR <ul> <li>FAccT 2024 - Latest fairness and accountability research</li> <li>FAccT 2023 - Includes algorithmic auditing advances</li> <li>FAccT 2022 - Focus on intersectionality and bias</li> </ul> <ul> <li>Focus on theoretical foundations and scalable algorithms</li> <li>Strong representation in robustness and privacy research</li> <li>Recent emphasis on LLM safety and alignment</li> </ul> <ul> <li>Cutting-edge deep learning approaches to trustworthy ML</li> <li>Novel architectures for interpretable models</li> <li>Adversarial robustness innovations</li> </ul>"},{"location":"research/papers/#specialized-venues","title":"Specialized Venues","text":"<ul> <li>AIES (AI, Ethics, and Society): Interdisciplinary perspectives</li> <li>S&amp;P, CCS, USENIX Security: Security and privacy focus  </li> <li>CHI, CSCW: Human-computer interaction and social impacts</li> <li>AAAI: Broad AI applications and theoretical work</li> </ul>"},{"location":"research/papers/#reading-lists-by-course-module","title":"Reading Lists by Course Module","text":""},{"location":"research/papers/#for-assignment-1-bias-detection","title":"For Assignment 1: Bias Detection","text":"<ol> <li>Verma &amp; Rubin (2018) - Fairness definitions overview</li> <li>Bellamy et al. (2019) - Practical fairness toolkit usage</li> <li>Choose one: Group fairness vs. individual fairness comparison</li> </ol>"},{"location":"research/papers/#for-assignment-2-adversarial-robustness","title":"For Assignment 2: Adversarial Robustness","text":"<ol> <li>Goodfellow et al. (2014) - FGSM and basic concepts</li> <li>Madry et al. (2017) - PGD and evaluation methodology</li> <li>Cohen et al. (2019) - Certified defenses introduction</li> </ol>"},{"location":"research/papers/#for-midterm-preparation","title":"For Midterm Preparation","text":"<p>Core papers from each topic area marked with \u2b50 in the full bibliography.</p> <p>Contributing</p> <p>Found an important paper we missed? Submit a suggestion or contribute directly to help keep this library comprehensive and current.</p> <p>Last updated: {{ git_revision_date }}</p>"},{"location":"resources/datasets/","title":"Datasets for Trustworthy ML","text":"<p>A curated collection of datasets commonly used for research and education in trustworthy machine learning.</p>"},{"location":"resources/datasets/#fairness-benchmarks","title":"Fairness Benchmarks","text":""},{"location":"resources/datasets/#traditional-ml-datasets","title":"Traditional ML Datasets","text":"<p>Adult Income Dataset UCI ML Repository | 48K samples | Tabular <pre><code>from aif360.datasets import AdultDataset\ndataset = AdultDataset()\n</code></pre> - Task: Income prediction (&gt;$50K/year) - Protected attributes: Race, gender, age - Use cases: Group fairness, bias detection - Notable papers: Most fairness papers use this dataset</p> <p>COMPAS Recidivism ProPublica | 7K samples | Tabular <pre><code>from aif360.datasets import CompasDataset  \ndataset = CompasDataset()\n</code></pre> - Task: Recidivism risk prediction - Protected attributes: Race, gender, age - Use cases: Criminal justice fairness, algorithmic auditing - Real-world impact: Used in actual court decisions</p>"},{"location":"resources/datasets/#modern-fairness-benchmarks","title":"Modern Fairness Benchmarks","text":"<p>Folktables Stanford | Millions of samples | Census data <pre><code>from folktables import ACSDataSource, ACSIncome\ndata_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n</code></pre> - Tasks: Income, employment, health insurance, travel time - Features: Realistic distribution shifts over time and geography - Best for: Large-scale fairness evaluation, intersectionality</p> <p>CelebA CUHK | 200K images | Face attributes <pre><code>import torchvision.datasets as datasets\ndataset = datasets.CelebA(root='./data', download=True)\n</code></pre> - Task: Multi-label face attribute prediction - Protected attributes: Gender, apparent age, race (inferred) - Use cases: Vision fairness, intersectional bias</p>"},{"location":"resources/datasets/#robustness-benchmarks","title":"Robustness Benchmarks","text":""},{"location":"resources/datasets/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>CIFAR-10/100 University of Toronto | 60K images | Object recognition <pre><code>import torchvision.datasets as datasets\ncifar10 = datasets.CIFAR10(root='./data', download=True)\n</code></pre> - Standard: Most common robustness benchmark - Attacks: FGSM, PGD, C&amp;W, AutoAttack - Defenses: Adversarial training, certified methods</p> <p>ImageNet Stanford | 14M images | Object recognition <pre><code>from torchvision.datasets import ImageNet\ndataset = ImageNet(root='./data', split='val')\n</code></pre> - Scale: Large-scale robustness evaluation - Use cases: Transfer learning robustness, real-world evaluation</p>"},{"location":"resources/datasets/#distribution-shift","title":"Distribution Shift","text":"<p>WILDS Stanford | Multiple domains | Distribution shift <pre><code>from wilds import get_dataset\ndataset = get_dataset(dataset=\"camelyon17\", download=True)\n</code></pre> - Datasets: Medical imaging, wildlife, satellite, text - Shifts: Geographic, temporal, demographic - Evaluation: Worst-group performance, average performance</p> <p>ImageNet-C UC Berkeley | 15 corruption types | Corrupted images <pre><code># Download from authors' website\nimport numpy as np\ncorrupt_data = np.load('imagenet_c/gaussian_noise/5/') \n</code></pre> - Corruptions: Weather, blur, noise, digital artifacts - Severity: 5 levels of corruption intensity - Use cases: Natural robustness evaluation</p>"},{"location":"resources/datasets/#privacy-benchmarks","title":"Privacy Benchmarks","text":""},{"location":"resources/datasets/#federated-learning","title":"Federated Learning","text":"<p>LEAF CMU | Multiple tasks | Federated setting <pre><code># Use LEAF data loaders\nfrom leaf.data_utils import read_data\nclients, groups, data = read_data('femnist')\n</code></pre> - Datasets: FEMNIST, CelebA, Reddit, Shakespeare - Properties: Non-IID data distribution, realistic client heterogeneity - Use cases: Federated learning algorithms, privacy evaluation</p> <p>FLamby Inria | Medical data | Cross-silo FL <pre><code>from flamby.datasets.fed_heart_disease import FedHeartDisease\ndataset = FedHeartDisease(center=0, train=True)\n</code></pre> - Focus: Medical federated learning - Datasets: Heart disease, skin cancer, drug discovery - Realistic: Based on real medical collaborations</p>"},{"location":"resources/datasets/#differential-privacy","title":"Differential Privacy","text":"<p>Adult + DP Mechanisms Google | Various | DP training examples <pre><code>from tensorflow_privacy.privacy.optimizers import dp_optimizer\noptimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(\n    l2_norm_clip=1.0, noise_multiplier=1.1, learning_rate=0.01)\n</code></pre> - Benchmarks: Standard datasets with DP training - Metrics: Privacy budget vs. accuracy trade-offs</p>"},{"location":"resources/datasets/#interpretability-datasets","title":"Interpretability Datasets","text":""},{"location":"resources/datasets/#feature-attribution","title":"Feature Attribution","text":"<p>Boston Housing UCI | 506 samples | Regression <pre><code>from sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)  # Note: deprecated due to ethical concerns\n</code></pre> - Use cases: Feature importance, explanation evaluation - Note: Consider alternatives due to ethical concerns</p> <p>Wine Quality UCI | 6K samples | Regression/Classification <pre><code>import pandas as pd\ndata = pd.read_csv('winequality-red.csv', delimiter=';')\n</code></pre> - Features: Chemical properties affecting wine quality - Use cases: Feature attribution, counterfactual explanations</p>"},{"location":"resources/datasets/#computer-vision","title":"Computer Vision","text":"<p>ImageNet + Attribution Google | 14M images | Saliency evaluation <pre><code>import saliency.core as saliency\ngradient_saliency = saliency.GradientSaliency()\n</code></pre> - Methods: GradCAM, Integrated Gradients, LIME - Evaluation: Pointing game, deletion/insertion metrics</p>"},{"location":"resources/datasets/#specialized-domains","title":"Specialized Domains","text":""},{"location":"resources/datasets/#natural-language-processing","title":"Natural Language Processing","text":"<p>BOLD Amazon | Text generation | Bias evaluation <pre><code>from bold import BoldDataset\ndataset = BoldDataset()\n</code></pre> - Focus: Bias in open-ended text generation - Attributes: Gender, race, religion, political ideology - Metrics: Sentiment, toxicity, regard</p> <p>Winogender Johns Hopkins | Coreference | Gender bias <pre><code>import json\nwith open('data/winogender-schemas.txt') as f:\n    templates = f.readlines()\n</code></pre> - Task: Pronoun coreference resolution - Bias: Gender stereotypes in occupations - Use cases: NLP fairness evaluation</p>"},{"location":"resources/datasets/#healthcare","title":"Healthcare","text":"<p>MIMIC-III MIT | 40K patients | Clinical records <pre><code># Requires credentialed access\nimport pandas as pd\nadmissions = pd.read_csv('ADMISSIONS.csv')\n</code></pre> - Access: Requires training and approval - Use cases: Healthcare fairness, privacy-preserving ML - Protected attributes: Race, gender, insurance</p>"},{"location":"resources/datasets/#finance","title":"Finance","text":"<p>German Credit UCI | 1K samples | Credit risk <pre><code>from aif360.datasets import GermanDataset\ndataset = GermanDataset()\n</code></pre> - Task: Credit risk assessment - Protected attributes: Age, gender, foreign worker status - Use cases: Financial fairness, regulatory compliance</p>"},{"location":"resources/datasets/#dataset-usage-guidelines","title":"Dataset Usage Guidelines","text":""},{"location":"resources/datasets/#fairness-analysis","title":"Fairness Analysis","text":"<ol> <li>Identify protected attributes in the dataset</li> <li>Define fairness metrics appropriate for the task  </li> <li>Evaluate intersectional effects across multiple attributes</li> <li>Consider historical bias in data collection</li> </ol>"},{"location":"resources/datasets/#robustness-testing","title":"Robustness Testing","text":"<ol> <li>Start with clean accuracy as baseline</li> <li>Apply systematic attacks with increasing strength</li> <li>Test multiple threat models (white-box, black-box)</li> <li>Evaluate on distribution shifts relevant to deployment</li> </ol>"},{"location":"resources/datasets/#privacy-evaluation","title":"Privacy Evaluation","text":"<ol> <li>Implement membership inference attacks as baseline</li> <li>Measure privacy-utility trade-offs across \u03b5 values</li> <li>Test reconstruction attacks where applicable</li> <li>Validate privacy guarantees with formal analysis</li> </ol>"},{"location":"resources/datasets/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"resources/datasets/#data-usage","title":"Data Usage","text":"<ul> <li>Consent: Ensure appropriate consent for research use</li> <li>Bias: Acknowledge limitations and potential biases</li> <li>Privacy: Follow data protection regulations (GDPR, etc.)</li> <li>Attribution: Cite original data sources appropriately</li> </ul>"},{"location":"resources/datasets/#sensitive-attributes","title":"Sensitive Attributes","text":"<ul> <li>Protected characteristics: Handle race, gender, etc. with care</li> <li>Intersectionality: Consider multiple overlapping identities</li> <li>Historical context: Understand societal biases in data</li> <li>Representation: Ensure diverse and inclusive datasets</li> </ul> <p>Dataset Deprecations</p> <p>Some datasets (e.g., Boston Housing) have been deprecated due to ethical concerns. Always check for recommended alternatives and consider the ethical implications of your dataset choices.</p> <p>Last updated: December 2024</p>"},{"location":"resources/tools/","title":"Tools &amp; Frameworks","text":"<p>A comprehensive collection of open-source tools, libraries, and frameworks for implementing trustworthy machine learning systems.</p>"},{"location":"resources/tools/#fairness-bias-mitigation","title":"Fairness &amp; Bias Mitigation","text":""},{"location":"resources/tools/#comprehensive-toolkits","title":"Comprehensive Toolkits","text":"<p>AI Fairness 360 (AIF360) IBM Research | Python, R | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install aif360\n</code></pre> - Features: 70+ fairness metrics, 10+ bias mitigation algorithms - Best for: Research, comprehensive bias analysis, enterprise applications - Highlights: Web interface, extensive documentation, industry-tested - Example: Credit scoring, hiring decisions, criminal justice</p> <p>Fairlearn Microsoft | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install fairlearn\n</code></pre> - Features: Scikit-learn integration, dashboard visualization, constraint-based optimization - Best for: Quick prototyping, ML practitioners familiar with sklearn - Highlights: User-friendly API, interactive dashboards, Azure ML integration</p>"},{"location":"resources/tools/#specialized-libraries","title":"Specialized Libraries","text":"<p>Themis UMass Amherst | Python | \u2b50\u2b50\u2b50 <pre><code>pip install themis-ml\n</code></pre> - Focus: Fairness testing and debugging - Features: Automated bias discovery, causal fairness analysis - Best for: Testing existing models for hidden biases</p> <p>FairML Academic | Python | \u2b50\u2b50\u2b50 <pre><code>pip install fairml\n</code></pre> - Focus: Auditing black-box models - Features: Input influence ranking, bias detection without model access - Best for: Third-party model auditing</p>"},{"location":"resources/tools/#robustness-adversarial-defense","title":"Robustness &amp; Adversarial Defense","text":""},{"location":"resources/tools/#attack-libraries","title":"Attack Libraries","text":"<p>Adversarial Robustness Toolbox (ART) IBM Research | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install adversarial-robustness-toolbox\n</code></pre> - Features: 20+ attacks, 15+ defenses, multiple ML frameworks - Frameworks: TensorFlow, PyTorch, scikit-learn, XGBoost - Best for: Comprehensive adversarial ML research and testing</p> <p>Foolbox University of T\u00fcbingen | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install foolbox\n</code></pre> - Features: 30+ gradient-based and black-box attacks - Frameworks: PyTorch, TensorFlow, JAX, NumPy - Best for: Quick adversarial example generation, benchmarking</p> <p>CleverHans Google Brain | Python | \u2b50\u2b50\u2b50 <pre><code>pip install cleverhans\n</code></pre> - Features: Classic attacks (FGSM, PGD, C&amp;W), TensorFlow focus - Best for: Educational purposes, reproducing classic papers</p>"},{"location":"resources/tools/#defense-frameworks","title":"Defense Frameworks","text":"<p>CROWN UCLA | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>git clone https://github.com/huanzhang12/CROWN-IBP\n</code></pre> - Focus: Certified robustness via interval bound propagation - Features: Formal verification, scalable certified training - Best for: Safety-critical applications requiring guarantees</p> <p>Auto-Attack EPFL | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install autoattack\n</code></pre> - Focus: Robust evaluation standard - Features: Ensemble of complementary attacks, parameter-free - Best for: Standardized robustness evaluation</p>"},{"location":"resources/tools/#interpretability-explainability","title":"Interpretability &amp; Explainability","text":""},{"location":"resources/tools/#model-agnostic-tools","title":"Model-Agnostic Tools","text":"<p>SHAP Microsoft Research | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install shap\n</code></pre> - Theory: Shapley values from cooperative game theory - Features: Global/local explanations, 15+ explainer types - Visualization: Interactive plots, force plots, dependence plots - Best for: Production explanations, business stakeholders</p> <p>LIME University of Washington | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install lime\n</code></pre> - Theory: Local linear approximation - Features: Text, image, tabular data support - Best for: Quick local explanations, diverse data types</p>"},{"location":"resources/tools/#deep-learning-specific","title":"Deep Learning Specific","text":"<p>Captum PyTorch Team | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install captum\n</code></pre> - Framework: Native PyTorch integration - Features: 15+ attribution algorithms, neuron/layer analysis - Visualization: Built-in visualization utilities - Best for: Deep learning research, PyTorch users</p> <p>Alibi Seldon | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install alibi\n</code></pre> - Features: Counterfactual explanations, anchor explanations - Focus: Production-ready explanations for ML deployment - Best for: Model serving, real-time explanations</p> <p>InterpretML Microsoft Research | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install interpret\n</code></pre> - Features: Glass-box models (EBM), model-agnostic explanations - Visualization: Unified dashboard for multiple explanation types - Best for: Regulated industries, healthcare, finance</p>"},{"location":"resources/tools/#privacy-preserving-ml","title":"Privacy-Preserving ML","text":""},{"location":"resources/tools/#differential-privacy","title":"Differential Privacy","text":"<p>Opacus PyTorch Team | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install opacus\n</code></pre> - Framework: PyTorch-native differential privacy - Features: DP-SGD, privacy accounting, gradient clipping - Best for: Deep learning with formal privacy guarantees</p> <p>TensorFlow Privacy Google | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install tensorflow-privacy\n</code></pre> - Framework: TensorFlow integration - Features: DP optimizers, privacy analysis, membership inference - Best for: Large-scale DP training, Google Cloud integration</p> <p>Diffprivlib IBM Research | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install diffprivlib\n</code></pre> - Framework: Scikit-learn compatible DP algorithms - Features: DP versions of common ML algorithms - Best for: Traditional ML with differential privacy</p>"},{"location":"resources/tools/#federated-learning","title":"Federated Learning","text":"<p>PySyft OpenMined | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install syft\n</code></pre> - Features: Federated learning, secure multi-party computation - Frameworks: PyTorch, TensorFlow support - Best for: Research, privacy-preserving collaborations</p> <p>Flower (flwr) Flower Labs | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install flwr\n</code></pre> - Features: Framework-agnostic federated learning - Deployment: Easy client-server architecture - Best for: Production federated learning, cross-platform deployment</p> <p>FedML FedML Inc | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install fedml\n</code></pre> - Features: MLOps for federated learning, mobile deployment - Platform: Cloud platform + open source library - Best for: End-to-end federated ML solutions</p>"},{"location":"resources/tools/#evaluation-benchmarking","title":"Evaluation &amp; Benchmarking","text":""},{"location":"resources/tools/#robustness-benchmarks","title":"Robustness Benchmarks","text":"<p>RobustBench Community | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install robustbench\n</code></pre> - Features: Standardized robustness evaluation, model zoo - Datasets: CIFAR-10/100, ImageNet, common corruptions - Best for: Comparing robustness methods, reproducible evaluation</p>"},{"location":"resources/tools/#fairness-benchmarks","title":"Fairness Benchmarks","text":"<p>Folktables Stanford | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install folktables\n</code></pre> - Features: Real-world fairness benchmarks from US Census data - Tasks: Income prediction, employment, health insurance - Best for: Realistic fairness evaluation, policy research</p>"},{"location":"resources/tools/#development-deployment","title":"Development &amp; Deployment","text":""},{"location":"resources/tools/#mlops-for-trustworthy-ml","title":"MLOps for Trustworthy ML","text":"<p>Evidently Evidently AI | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install evidently\n</code></pre> - Features: ML monitoring, drift detection, bias monitoring - Deployment: Dashboard, reports, real-time monitoring - Best for: Production ML monitoring, continuous auditing</p> <p>Great Expectations Superconductive | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install great-expectations\n</code></pre> - Features: Data validation, pipeline testing, documentation - Integration: Airflow, dbt, cloud platforms - Best for: Data quality assurance, ML pipeline validation</p>"},{"location":"resources/tools/#model-cards-documentation","title":"Model Cards &amp; Documentation","text":"<p>Model Card Toolkit Google | Python | \u2b50\u2b50\u2b50 <pre><code>pip install model-card-toolkit\n</code></pre> - Features: Automated model card generation, templates - Integration: TensorFlow Model Analysis integration - Best for: Model documentation, regulatory compliance</p>"},{"location":"resources/tools/#quick-start-guides","title":"Quick Start Guides","text":""},{"location":"resources/tools/#fairness-analysis-workflow","title":"Fairness Analysis Workflow","text":"<pre><code># Using AIF360 for comprehensive bias analysis\nfrom aif360.datasets import AdultDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.algorithms.preprocessing import Reweighing\n\n# Load data and compute bias metrics\ndataset = AdultDataset()\nmetric = BinaryLabelDatasetMetric(dataset)\nprint(f\"Disparate Impact: {metric.disparate_impact()}\")\n\n# Apply bias mitigation\nrw = Reweighing(unprivileged_groups=[{'sex': 0}],\n                privileged_groups=[{'sex': 1}])\ndataset_transf = rw.fit_transform(dataset)\n</code></pre>"},{"location":"resources/tools/#adversarial-robustness-testing","title":"Adversarial Robustness Testing","text":"<pre><code># Using ART for adversarial evaluation\nfrom art.attacks.evasion import FastGradientMethod\nfrom art.estimators.classification import KerasClassifier\n\n# Wrap your model\nclassifier = KerasClassifier(model=model)\n\n# Generate adversarial examples\nattack = FastGradientMethod(estimator=classifier, eps=0.1)\nx_test_adv = attack.generate(x=x_test)\n\n# Evaluate robustness\naccuracy_clean = classifier.predict(x_test).argmax(axis=1) == y_test\naccuracy_adv = classifier.predict(x_test_adv).argmax(axis=1) == y_test\nprint(f\"Clean accuracy: {accuracy_clean.mean():.2f}\")\nprint(f\"Adversarial accuracy: {accuracy_adv.mean():.2f}\")\n</code></pre>"},{"location":"resources/tools/#privacy-preserving-training","title":"Privacy-Preserving Training","text":"<pre><code># Using Opacus for differential privacy\nfrom opacus import PrivacyEngine\n\n# Attach privacy engine to optimizer\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, data_loader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=data_loader,\n    noise_multiplier=1.0,\n    max_grad_norm=1.0,\n)\n\n# Train with privacy guarantees\nfor epoch in range(epochs):\n    for batch in data_loader:\n        # Standard PyTorch training loop\n        optimizer.zero_grad()\n        loss = criterion(model(batch[0]), batch[1])\n        loss.backward()\n        optimizer.step()\n\n    # Check privacy budget\n    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n    print(f\"Epoch {epoch}, \u03b5 = {epsilon:.2f}\")\n</code></pre> <p>Tool Selection Guide</p> <ul> <li>Research: Start with comprehensive toolkits (AIF360, ART, SHAP)</li> <li>Production: Focus on framework-specific tools (Fairlearn for sklearn, Captum for PyTorch)</li> <li>Evaluation: Use standardized benchmarks (RobustBench, Folktables)</li> <li>Deployment: Implement monitoring (Evidently, Great Expectations)</li> </ul> <p>Last updated: December 2024</p>"}]}