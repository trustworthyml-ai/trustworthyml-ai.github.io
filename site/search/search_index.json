{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Trustworthy Machine Learning","text":"<p>Welcome to the comprehensive resource hub for Trustworthy Machine Learning (TML). This site serves as a central repository for course materials, cutting-edge research, and community resources in the rapidly evolving field of trustworthy AI.</p>"},{"location":"#what-is-trustworthy-ml","title":"What is Trustworthy ML?","text":"<p>Trustworthy Machine Learning encompasses the principles and practices needed to build AI systems that are:</p> <ul> <li>Fair and unbiased across different populations</li> <li>Robust to adversarial attacks and distribution shifts  </li> <li>Transparent and interpretable in their decision-making</li> <li>Privacy-preserving in how they handle sensitive data</li> <li>Accountable for their predictions and recommendations</li> </ul>"},{"location":"#course-overview","title":"Course Overview","text":"<p>Our comprehensive course covers the fundamental concepts, state-of-the-art techniques, and practical implementations of trustworthy ML systems. Whether you're a student, researcher, or practitioner, you'll find valuable resources here.</p> <p>Fall 2025 Course</p> <p>The Trustworthy ML course is being offered in Fall 2025 at UCLA Extension. Check out the syllabus for detailed information about topics, schedule, and assignments, or enroll directly.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"StudentsResearchersPractitioners <ul> <li>Course Syllabus</li> <li>Schedule &amp; Lectures </li> <li>Assignments</li> <li>Project Guidelines</li> </ul> <ul> <li>Paper Library</li> <li>Key Topics</li> <li>Recent Advances</li> </ul> <ul> <li>Tools &amp; Frameworks</li> <li>Datasets</li> <li>Tutorials</li> </ul>"},{"location":"#featured-topics","title":"Featured Topics","text":"<ul> <li>Fairness in AI: Bias detection, mitigation strategies, and fairness metrics</li> <li>Adversarial Robustness: Defense mechanisms and robust training techniques</li> <li>Explainable AI: Interpretability methods and transparency tools</li> <li>Privacy-Preserving ML: Differential privacy, federated learning, and secure computation</li> <li>AI Safety: Alignment, safety verification, and risk assessment</li> </ul>"},{"location":"#stay-connected","title":"Stay Connected","text":"<p>Connect with the trustworthy ML community:</p> <ul> <li>\ud83d\udc19 GitHub Organization</li> <li>\ud83d\udc26 Twitter Updates</li> </ul> <p>This resource is continuously updated with the latest research, tools, and community contributions.</p>"},{"location":"about/","title":"About the Instructor","text":"Prashant Kulkarni <p>Lead AI Security Research Engineer, Google Cloud</p> <p>Master of Science in Applied Data Science, University of Chicago</p>"},{"location":"about/#professional-background","title":"Professional Background","text":"<p>With over 20 years of cybersecurity experience, Prashant Kulkarni is a leading expert in AI security and machine learning safety. As a Lead AI Security Research Engineer at Google Cloud, he focuses on securing modern AI systems, including large language models, and collaborates extensively with customers on AI system security.</p> <p>His expertise spans the critical intersection of cybersecurity and artificial intelligence, making him uniquely positioned to teach trustworthy machine learning principles that are both theoretically sound and practically applicable in today's rapidly evolving AI landscape.</p>"},{"location":"about/#research-expertise","title":"Research &amp; Expertise","text":"<p>Prashant's research interests and professional focus areas include:</p>"},{"location":"about/#ai-security","title":"\ud83d\udee1\ufe0f AI Security","text":"<ul> <li>Securing large language models and generative AI systems</li> <li>Threat modeling for AI/ML pipelines</li> <li>Security-by-design for AI applications</li> </ul>"},{"location":"about/#adversarial-defenses","title":"\ud83d\udd12 Adversarial Defenses","text":"<ul> <li>Robust training techniques against adversarial attacks</li> <li>Certified defense mechanisms</li> <li>Real-world robustness evaluation</li> </ul>"},{"location":"about/#privacy-preserving-ml","title":"\ud83d\udd10 Privacy-Preserving ML","text":"<ul> <li>Differential privacy in production systems</li> <li>Federated learning security</li> <li>Privacy-utility trade-offs in AI systems</li> </ul>"},{"location":"about/#ethical-ai-deployment","title":"\u2696\ufe0f Ethical AI Deployment","text":"<ul> <li>Responsible AI practices in enterprise environments</li> <li>Bias detection and mitigation at scale</li> <li>AI governance and compliance frameworks</li> </ul>"},{"location":"about/#teaching-philosophy","title":"Teaching Philosophy","text":"<p>\"Simplifying complex concepts and fostering hands-on learning, enabling participants to grasp intricate topics in responsible AI\"</p> <p>Prashant is passionate about empowering adult learners and believes in making complex technical concepts accessible through:</p> <ul> <li>Practical Application: Every theoretical concept is paired with hands-on implementation</li> <li>Real-World Context: Examples drawn from actual industry challenges and solutions</li> <li>Interactive Learning: Encouraging questions, discussions, and collaborative problem-solving</li> <li>Industry Relevance: Focus on skills and knowledge directly applicable to professional work</li> </ul>"},{"location":"about/#academic-credentials","title":"Academic Credentials","text":"<p>Master of Science in Applied Data Science University of Chicago</p> <p>Prashant's advanced degree in Applied Data Science provides him with both the theoretical foundation and practical experience necessary to teach the mathematical and statistical underpinnings of trustworthy ML while maintaining focus on real-world applications.</p>"},{"location":"about/#professional-experience","title":"Professional Experience","text":""},{"location":"about/#current-role-lead-ai-security-research-engineer","title":"Current Role: Lead AI Security Research Engineer","text":"<p>Google Cloud | Present</p> <ul> <li>Leads research initiatives in AI security and safety</li> <li>Develops security frameworks for enterprise AI deployments</li> <li>Collaborates with product teams on secure AI system design</li> <li>Works directly with customers on AI security implementations</li> </ul>"},{"location":"about/#20-years-in-cybersecurity","title":"20+ Years in Cybersecurity","text":"<p>Throughout his career, Prashant has:</p> <ul> <li>Developed security solutions for complex, large-scale systems</li> <li>Led teams in implementing robust security practices</li> <li>Advised organizations on emerging security threats</li> <li>Published research on cybersecurity and AI safety</li> </ul>"},{"location":"about/#course-trustworthy-machine-learning","title":"Course: Trustworthy Machine Learning","text":"<p>UCLA Extension | Fall 2025</p> <p>Prashant teaches Trustworthy Machine Learning at UCLA Extension, providing a comprehensive introduction to building AI systems that are fair, robust, transparent, and secure.</p> <p>The course combines: - Theoretical Foundations: Mathematical frameworks for trustworthy AI - Practical Implementation: Hands-on labs with industry-standard tools - Real-World Case Studies: Examples from actual deployments and failures - Current Research: Latest developments in the field</p>"},{"location":"about/#course-highlights","title":"Course Highlights","text":"<ul> <li>Small class sizes for personalized attention</li> <li>Industry-relevant assignments and projects</li> <li>Guest speakers from leading tech companies</li> <li>Networking opportunities with professionals in the field</li> </ul>"},{"location":"about/#industry-impact","title":"Industry Impact","text":"<p>Prashant's work has contributed to:</p> <ul> <li>Secure AI Frameworks used by enterprise customers at Google Cloud</li> <li>Best Practices for AI security in production environments  </li> <li>Research Publications on trustworthy AI and security</li> <li>Training Programs for AI practitioners and security professionals</li> </ul>"},{"location":"about/#connect-learn-more","title":"Connect &amp; Learn More","text":""},{"location":"about/#professional-profiles","title":"Professional Profiles","text":"<ul> <li>LinkedIn: Connect with Prashant for professional updates and insights</li> <li>Google Scholar: Follow research publications</li> <li>GitHub: Explore open-source contributions to trustworthy AI tools</li> </ul>"},{"location":"about/#speaking-consulting","title":"Speaking &amp; Consulting","text":"<p>Prashant is available for: - Conference presentations on AI security and trustworthy ML - Corporate training workshops - Consulting on AI security strategy and implementation - Guest lectures at academic institutions</p>"},{"location":"about/#course-information","title":"Course Information","text":"<p>Interested in learning from Prashant? </p> <p>\ud83d\udcda Enroll in Trustworthy Machine Learning at UCLA Extension</p> <ul> <li>Format: Online only</li> <li>Duration: 11-week intensive course</li> <li>Prerequisites: Basic ML knowledge, Python programming</li> <li>Certification: UCLA Extension certificate upon completion</li> </ul>"},{"location":"about/#research-publications","title":"Research &amp; Publications","text":"<p>Prashant's research contributions span cybersecurity, AI safety, and trustworthy machine learning. His work bridges the gap between theoretical advances and practical implementations in enterprise environments.</p>"},{"location":"about/#research-areas","title":"Research Areas","text":"<ul> <li>AI Security &amp; Safety: Securing large language models and generative AI systems</li> <li>Adversarial Machine Learning: Robust defenses against adversarial attacks</li> <li>Privacy-Preserving ML: Differential privacy and federated learning</li> <li>Trustworthy AI: Fairness, accountability, and transparency in AI systems</li> </ul>"},{"location":"about/#selected-publications","title":"Selected Publications","text":"<p>Google Scholar Profile</p> <p>For a complete list of publications and citation metrics, visit Prashant's Google Scholar profile.</p> <p>Recent Research Focus: - Security frameworks for enterprise AI deployments - Practical implementations of differential privacy - Adversarial robustness in production ML systems - Bias detection and mitigation at scale</p>"},{"location":"about/#research-impact","title":"Research Impact","text":"<ul> <li>Developed security guidelines adopted by industry practitioners</li> <li>Contributed to open-source tools for trustworthy AI</li> <li>Published research on practical AI safety implementations</li> <li>Collaborated with academic institutions on AI security research</li> </ul> <p>For speaking engagements, consulting inquiries, or course-related questions, please use the contact information available through UCLA Extension or connect via professional networks.</p>"},{"location":"blog/","title":"Trustworthy ML Blog","text":"<p>Welcome to our blog featuring insights, tutorials, and updates from the trustworthy ML community.</p> <p>Coming Soon</p> <p>The blog section is under development. We'll be featuring:</p> <ul> <li>Student project highlights</li> <li>Research paper summaries  </li> <li>Industry case studies</li> <li>Tool tutorials and reviews</li> <li>Community spotlights</li> </ul>"},{"location":"blog/#planned-content","title":"Planned Content","text":""},{"location":"blog/#research-spotlights","title":"Research Spotlights","text":"<p>Deep dives into important papers and their implications for practitioners.</p>"},{"location":"blog/#tool-reviews","title":"Tool Reviews","text":"<p>Hands-on reviews of new tools and frameworks in the trustworthy ML ecosystem.</p>"},{"location":"blog/#case-studies","title":"Case Studies","text":"<p>Real-world applications of trustworthy ML principles in industry.</p>"},{"location":"blog/#guest-posts","title":"Guest Posts","text":"<p>Contributions from researchers, practitioners, and students in the field.</p> <p>Stay tuned for regular updates and new content!</p>"},{"location":"course/assignments/","title":"Assignments","text":"<p>Detailed information about course assignments, including specifications, submission guidelines, and grading rubrics.</p> <p>Under Development</p> <p>Assignment details will be posted here as they are released throughout the semester.</p>"},{"location":"course/assignments/#assignment-overview","title":"Assignment Overview","text":"Assignment Topic Weight Release Due A1 Bias Detection &amp; Mitigation 10% Week 4 Week 6 A2 Adversarial Robustness 10% Week 7 Week 9 A3 Model Interpretability 10% Week 10 Week 11 A4 Privacy-Preserving ML 10% Week 12 Week 14"},{"location":"course/assignments/#assignment-specifications","title":"Assignment Specifications","text":""},{"location":"course/assignments/#assignment-1-bias-detection-mitigation","title":"Assignment 1: Bias Detection &amp; Mitigation","text":"<p>Release: Week 4 | Due: Week 6</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-2-adversarial-robustness","title":"Assignment 2: Adversarial Robustness","text":"<p>Release: Week 7 | Due: Week 9</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-3-model-interpretability","title":"Assignment 3: Model Interpretability","text":"<p>Release: Week 10 | Due: Week 11</p> <p>Coming soon...</p>"},{"location":"course/assignments/#assignment-4-privacy-preserving-ml","title":"Assignment 4: Privacy-Preserving ML","text":"<p>Release: Week 12 | Due: Week 14</p> <p>Coming soon...</p>"},{"location":"course/assignments/#submission-guidelines","title":"Submission Guidelines","text":"<ul> <li>Submit via course management system</li> <li>Include all code, data, and documentation</li> <li>Follow academic integrity policies</li> <li>Late penalty: 10% per day</li> </ul> <p>This page will be updated as assignments are released.</p>"},{"location":"course/projects/","title":"Final Projects","text":"<p>The final project is a team-based research project where you'll explore an open problem in trustworthy machine learning.</p>"},{"location":"course/projects/#project-overview","title":"Project Overview","text":"<p>Team Size: 3-4 students Timeline: 8 weeks (Weeks 8-15) Weight: 30% of final grade  </p>"},{"location":"course/projects/#project-components","title":"Project Components","text":"Component Weight Due Date Proposal 5% Week 11 Progress Report 5% Week 13 Final Presentation 10% Week 15 Final Report 10% Finals Week"},{"location":"course/projects/#project-guidelines","title":"Project Guidelines","text":""},{"location":"course/projects/#scope-and-topics","title":"Scope and Topics","text":"<p>Your project should address a research question in one or more areas of trustworthy ML:</p> <ul> <li>Fairness &amp; Bias: Novel detection methods, mitigation algorithms, evaluation metrics</li> <li>Robustness: New attack techniques, defense mechanisms, certified methods  </li> <li>Interpretability: Explanation methods, evaluation frameworks, human studies</li> <li>Privacy: Differential privacy mechanisms, federated learning innovations</li> <li>Safety &amp; Alignment: Value learning, reward modeling, verification methods</li> </ul>"},{"location":"course/projects/#requirements","title":"Requirements","text":"<ul> <li>Novelty: Propose new methods or provide new insights</li> <li>Implementation: Working code with experiments</li> <li>Evaluation: Rigorous experimental validation</li> <li>Writing: Clear technical exposition</li> </ul>"},{"location":"course/projects/#deliverables","title":"Deliverables","text":""},{"location":"course/projects/#project-proposal-2-pages","title":"Project Proposal (2 pages)","text":"<p>Due: Week 11</p> <p>Required Sections: 1. Problem Statement: What challenge are you addressing? 2. Related Work: Brief survey of relevant papers (5-10 papers) 3. Approach: Your proposed method or analysis 4. Evaluation Plan: Datasets, metrics, baselines 5. Timeline: Milestones for remaining weeks</p>"},{"location":"course/projects/#progress-report-1-page","title":"Progress Report (1 page)","text":"<p>Due: Week 13</p> <p>Required Sections: 1. Progress Summary: What you've completed 2. Preliminary Results: Initial findings or implementation 3. Challenges: Issues encountered and solutions 4. Updated Timeline: Revised plan for final weeks</p>"},{"location":"course/projects/#final-presentation-15-minutes","title":"Final Presentation (15 minutes)","text":"<p>Date: Week 15</p> <p>Presentation Structure: - Problem motivation (2-3 min) - Technical approach (5-6 min) - Experimental results (4-5 min) - Conclusions and future work (2-3 min) - Q&amp;A (5 min)</p>"},{"location":"course/projects/#final-report-8-10-pages","title":"Final Report (8-10 pages)","text":"<p>Due: Finals Week</p> <p>Required Sections: 1. Abstract: Problem, approach, key findings 2. Introduction: Motivation and problem statement 3. Related Work: Comprehensive literature review 4. Method: Detailed technical description 5. Experiments: Setup, results, analysis 6. Discussion: Limitations, implications, future work 7. Conclusion: Summary of contributions</p>"},{"location":"course/projects/#project-ideas","title":"Project Ideas","text":""},{"location":"course/projects/#fairness-projects","title":"Fairness Projects","text":"<ul> <li>Intersectional Fairness: Methods for multi-attribute fairness</li> <li>Dynamic Fairness: Fairness that adapts over time</li> <li>Fairness in NLP: Bias detection in language models</li> <li>Causal Fairness: Using causal inference for fair decisions</li> </ul>"},{"location":"course/projects/#robustness-projects","title":"Robustness Projects","text":"<ul> <li>Universal Adversarial Perturbations: Domain-specific attacks</li> <li>Certified Defense: Improving scalability of verification</li> <li>Real-World Robustness: Robustness to natural distribution shifts</li> <li>Robust Federated Learning: Security in distributed training</li> </ul>"},{"location":"course/projects/#interpretability-projects","title":"Interpretability Projects","text":"<ul> <li>Counterfactual Explanations: Generating actionable explanations  </li> <li>Explanation Evaluation: New metrics for explanation quality</li> <li>Interactive Explanations: Human-in-the-loop explanation systems</li> <li>Interpretable Deep Learning: Inherently interpretable architectures</li> </ul>"},{"location":"course/projects/#privacy-projects","title":"Privacy Projects","text":"<ul> <li>Local Differential Privacy: Privacy without trusted curator</li> <li>Private Representation Learning: Privacy-preserving embeddings</li> <li>Membership Inference Defense: Protecting against privacy attacks</li> <li>Federated Learning Privacy: Novel privacy-utility trade-offs</li> </ul>"},{"location":"course/projects/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"course/projects/#technical-quality-40","title":"Technical Quality (40%)","text":"<ul> <li>Correctness: Sound methodology and implementation</li> <li>Novelty: Original insights or approaches  </li> <li>Rigor: Thorough experimental validation</li> <li>Reproducibility: Clear implementation details</li> </ul>"},{"location":"course/projects/#presentation-30","title":"Presentation (30%)","text":"<ul> <li>Clarity: Clear problem statement and solution</li> <li>Organization: Logical flow and structure</li> <li>Delivery: Effective oral presentation skills</li> <li>Visual Design: Quality figures and slides</li> </ul>"},{"location":"course/projects/#writing-30","title":"Writing (30%)","text":"<ul> <li>Technical Writing: Clear, precise exposition</li> <li>Related Work: Comprehensive literature coverage</li> <li>Analysis: Thoughtful discussion of results</li> <li>Formatting: Professional presentation</li> </ul>"},{"location":"course/projects/#resources","title":"Resources","text":""},{"location":"course/projects/#datasets","title":"Datasets","text":"<ul> <li>Fairness: Adult, COMPAS, CelebA, Folktables</li> <li>Robustness: CIFAR-10/100, ImageNet, MNIST</li> <li>Privacy: See federated learning benchmarks</li> <li>General: Papers with Datasets collections</li> </ul>"},{"location":"course/projects/#computing-resources","title":"Computing Resources","text":"<ul> <li>Google Colab Pro: For small-scale experiments</li> <li>Course Cluster: For larger computational needs</li> <li>Cloud Credits: Limited AWS/Azure credits available</li> </ul>"},{"location":"course/projects/#collaboration-tools","title":"Collaboration Tools","text":"<ul> <li>GitHub: Version control and collaboration</li> <li>Overleaf: Collaborative LaTeX writing</li> <li>Slack: Team communication</li> <li>Office Hours: Weekly project consultations</li> </ul>"},{"location":"course/projects/#timeline","title":"Timeline","text":""},{"location":"course/projects/#week-8-10-team-formation-topic-selection","title":"Week 8-10: Team Formation &amp; Topic Selection","text":"<ul> <li>Form teams and explore project ideas</li> <li>Read relevant papers and identify gaps</li> <li>Discuss ideas during office hours</li> </ul>"},{"location":"course/projects/#week-11-proposal-submission","title":"Week 11: Proposal Submission","text":"<ul> <li>Submit 2-page project proposal</li> <li>Receive feedback from instructors</li> </ul>"},{"location":"course/projects/#week-12-13-implementation-experiments","title":"Week 12-13: Implementation &amp; Experiments","text":"<ul> <li>Implement core methodology</li> <li>Run initial experiments</li> <li>Submit progress report</li> </ul>"},{"location":"course/projects/#week-14-final-push","title":"Week 14: Final Push","text":"<ul> <li>Complete experiments and analysis</li> <li>Prepare presentation slides</li> <li>Draft final report</li> </ul>"},{"location":"course/projects/#week-15-presentations","title":"Week 15: Presentations","text":"<ul> <li>Present findings to class</li> <li>Provide peer feedback</li> <li>Finalize written report</li> </ul>"},{"location":"course/projects/#past-project-examples","title":"Past Project Examples","text":""},{"location":"course/projects/#successful-projects-previous-years","title":"Successful Projects (Previous Years)","text":"<ul> <li>\"Fairness-Aware Multi-Task Learning\" - Novel algorithm with theoretical analysis</li> <li>\"Robust Vision Transformers\" - Comprehensive robustness evaluation</li> <li>\"Explaining Neural Recommendation Systems\" - Human evaluation study</li> <li>\"Privacy-Preserving Graph Neural Networks\" - DP mechanisms for graph data</li> </ul> <p>For questions about projects, contact the teaching team during office hours or via email.</p>"},{"location":"course/schedule/","title":"Course Schedule","text":"<p>Fall 2025 | Trustworthy Machine Learning</p> <p>Schedule Updates</p> <p>This schedule is subject to change. Check regularly for updates and assignment due dates.</p>"},{"location":"course/schedule/#week-by-week-schedule","title":"Week-by-Week Schedule","text":"Week Topic 1 Introduction to Trustworthy ML 2 Model Evaluation and Fairness 3 Midterm Project 4 Privacy Enhancing Technologies I 5 Privacy-Enhancing Technologies II &amp; Federated Learning 6 Gen AI Security Models and Frameworks 7 Safety, Alignment, and Evaluation in LLMs 8 Advanced LLM Safety 9 Security Testing and Red Teaming 10 AI Regulatory Frameworks and Compliance 11 Final Project"},{"location":"course/schedule/#reading-materials","title":"Reading Materials","text":""},{"location":"course/schedule/#supplementary-resources","title":"Supplementary Resources","text":"<ul> <li>Course slides and lecture notes</li> <li>Research papers (linked in weekly readings)</li> <li>Tutorial videos and demos</li> <li>Tool documentation and examples</li> </ul>"},{"location":"course/schedule/#office-hours","title":"Office Hours","text":"<ul> <li>Instructor: Tuesdays 2-4 PM, Fridays 1-2 PM</li> <li>TAs: </li> <li>TA1: Mondays 3-5 PM, Wednesdays 10-12 PM</li> <li>TA2: Thursdays 2-4 PM, Saturdays 10-12 PM</li> </ul> <p>Schedule last updated: {{ git_revision_date_localized }}</p>"},{"location":"course/syllabus/","title":"Course Syllabus: Trustworthy Machine Learning","text":""},{"location":"course/syllabus/#course-information","title":"Course Information","text":"<p>Course Title: Trustworthy Machine Learning Institution: UCLA Extension Semester: Fall 2025 Format: Online Only Duration: 11 weeks Course Link: COM SCI X 450.44 Prerequisites: Machine Learning fundamentals, Linear Algebra, Statistics, Python programming Credits: 3  </p>"},{"location":"course/syllabus/#course-description","title":"Course Description","text":"<p>This course provides a comprehensive introduction to the principles, methods, and applications of trustworthy machine learning. Students will learn to design, implement, and evaluate ML systems that are fair, robust, transparent, privacy-preserving, and accountable. The course combines theoretical foundations with hands-on projects using real-world datasets and modern tools.</p>"},{"location":"course/syllabus/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Understand Core Concepts: Define and explain the key principles of trustworthy ML including fairness, robustness, interpretability, privacy, and accountability</li> <li>Identify Vulnerabilities: Recognize potential sources of bias, adversarial attacks, and privacy leaks in ML systems</li> <li>Apply Mitigation Techniques: Implement state-of-the-art methods for bias mitigation, adversarial defense, and privacy protection</li> <li>Evaluate Systems: Use appropriate metrics and evaluation frameworks to assess the trustworthiness of ML models</li> <li>Design Solutions: Architect end-to-end trustworthy ML systems for real-world applications</li> </ol>"},{"location":"course/syllabus/#course-topics","title":"Course Topics","text":"Week Topic 1 Introduction to Trustworthy ML 2 Model Evaluation and Fairness 3 Midterm Project 4 Privacy Enhancing Technologies I 5 Privacy-Enhancing Technologies II &amp; Federated Learning 6 Gen AI Security Models and Frameworks 7 Safety, Alignment, and Evaluation in LLMs 8 Advanced LLM Safety 9 Security Testing and Red Teaming 10 AI Regulatory Frameworks and Compliance 11 Final Project"},{"location":"course/syllabus/#required-resources","title":"Required Resources","text":""},{"location":"course/syllabus/#software-tools","title":"Software &amp; Tools","text":"<ul> <li>Python 3.8+ with scikit-learn, PyTorch/TensorFlow</li> <li>Fairness toolkits: AIF360, Fairlearn</li> <li>Privacy libraries: Opacus, PySyft</li> <li>Interpretability tools: SHAP, LIME, Captum</li> </ul>"},{"location":"course/syllabus/#computing-resources","title":"Computing Resources","text":"<ul> <li>Google Colab Pro or similar cloud platform</li> </ul> <p>This syllabus is subject to change with advance notice. Check the course website regularly for updates.</p>"},{"location":"research/papers/","title":"Research Paper Library","text":"<p>A curated collection of seminal and recent papers in trustworthy machine learning. Papers are organized by topic and include our commentary on significance and practical implications.</p> <p>Search &amp; Filter</p> <p>Use <code>Ctrl+F</code> to search for specific topics, authors, or venues. Papers are tagged with key concepts for easy discovery.</p>"},{"location":"research/papers/#foundational-papers","title":"Foundational Papers","text":""},{"location":"research/papers/#fairness-bias","title":"Fairness &amp; Bias","text":"<p>Seminal Works</p> <ul> <li> <p>Fairness Through Awareness (Dwork et al., 2012) ITCS 2012 | <code>individual-fairness</code> <code>awareness</code>   Introduces the concept of individual fairness and awareness in algorithmic decision-making.</p> </li> <li> <p>Equality of Opportunity in Supervised Learning (Hardt et al., 2016) NIPS 2016 | <code>group-fairness</code> <code>equalized-odds</code>   Defines equalized odds and equality of opportunity for binary classification.</p> </li> <li> <p>Fairness Definitions Explained (Verma &amp; Rubin, 2018) IEEE FATES 2018 | <code>survey</code> <code>fairness-metrics</code>   Comprehensive survey of 20+ fairness definitions with mathematical formulations.</p> </li> </ul> <p>Recent Advances</p> <ul> <li> <p>Fairness-Aware Machine Learning: Practical Challenges and Lessons Learned (Bellamy et al., 2019) WSDM 2019 | <code>aif360</code> <code>toolkit</code> <code>industry</code>   Practical insights from deploying fairness-aware ML in enterprise settings.</p> </li> <li> <p>Intersectional Fairness: A Fractal Approach (Foulds et al., 2020) FAccT 2020 | <code>intersectionality</code> <code>subgroup-fairness</code>   Novel approach to handling fairness across intersecting protected attributes.</p> </li> </ul>"},{"location":"research/papers/#robustness-adversarial-ml","title":"Robustness &amp; Adversarial ML","text":"<p>Foundational</p> <ul> <li> <p>Intriguing Properties of Neural Networks (Szegedy et al., 2013) ICLR 2014 | <code>adversarial-examples</code> <code>discovery</code>   First systematic study of adversarial examples in deep neural networks.</p> </li> <li> <p>Explaining and Harnessing Adversarial Examples (Goodfellow et al., 2014) ICLR 2015 | <code>fgsm</code> <code>linear-hypothesis</code>   Introduces FGSM attack and linear hypothesis for adversarial vulnerability.</p> </li> <li> <p>Towards Deep Learning Models Resistant to Adversarial Attacks (Madry et al., 2017) ICLR 2018 | <code>pgd</code> <code>adversarial-training</code>   Establishes PGD as the gold standard for adversarial training evaluation.</p> </li> </ul> <p>Certified Defenses</p> <ul> <li> <p>Certified Adversarial Robustness via Randomized Smoothing (Cohen et al., 2019) ICML 2019 | <code>certified-defense</code> <code>randomized-smoothing</code>   Scalable approach to obtaining robustness certificates using Gaussian noise.</p> </li> <li> <p>Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers (Salman et al., 2019) NeurIPS 2019 | <code>certified-training</code> <code>smoothing</code>   Combines adversarial training with randomized smoothing for stronger guarantees.</p> </li> </ul>"},{"location":"research/papers/#interpretability-explainability","title":"Interpretability &amp; Explainability","text":"<p>Core Methods</p> <ul> <li> <p>\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier (Ribeiro et al., 2016) KDD 2016 | <code>lime</code> <code>local-explanations</code>   Introduces LIME for locally interpretable model-agnostic explanations.</p> </li> <li> <p>A Unified Approach to Interpreting Model Predictions (Lundberg &amp; Lee, 2017) NIPS 2017 | <code>shap</code> <code>shapley-values</code>   SHAP: Unified framework based on cooperative game theory.</p> </li> <li> <p>Attention is All You Need (Vaswani et al., 2017) NIPS 2017 | <code>attention</code> <code>transformers</code> <code>interpretability</code>   While primarily an architecture paper, attention mechanisms provide built-in interpretability.</p> </li> </ul> <p>Evaluation &amp; Benchmarking</p> <ul> <li> <p>Evaluating the Visualization of What a Deep Neural Network Has Learned (Simonyan et al., 2013) ICLR 2014 | <code>saliency-maps</code> <code>evaluation</code>   Early work on evaluating explanation quality through perturbation analysis.</p> </li> <li> <p>Sanity Checks for Saliency Maps (Adebayo et al., 2018) NeurIPS 2018 | <code>sanity-checks</code> <code>saliency-evaluation</code>   Demonstrates that many explanation methods fail basic sanity checks.</p> </li> </ul>"},{"location":"research/papers/#privacy-preserving-ml","title":"Privacy-Preserving ML","text":"<p>Differential Privacy</p> <ul> <li> <p>Deep Learning with Differential Privacy (Abadi et al., 2016) CCS 2016 | <code>differential-privacy</code> <code>sgd</code> <code>deep-learning</code>   First practical application of differential privacy to deep learning training.</p> </li> <li> <p>The Algorithmic Foundations of Differential Privacy (Dwork &amp; Roth, 2014) Foundations and Trends | <code>dp-foundations</code> <code>survey</code>   Comprehensive theoretical foundation of differential privacy.</p> </li> </ul> <p>Federated Learning</p> <ul> <li> <p>Communication-Efficient Learning of Deep Networks from Decentralized Data (McMahan et al., 2017) AISTATS 2017 | <code>federated-learning</code> <code>fedavg</code>   Introduces federated learning and the FedAvg algorithm.</p> </li> <li> <p>Towards Federated Learning at Scale: System Design (Bonawitz et al., 2019) MLSys 2019 | <code>federated-systems</code> <code>scale</code>   System design considerations for large-scale federated learning deployment.</p> </li> </ul>"},{"location":"research/papers/#recent-research-2023-2024","title":"Recent Research (2023-2024)","text":""},{"location":"research/papers/#emerging-topics","title":"Emerging Topics","text":"<ul> <li> <p>Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022) Anthropic | <code>constitutional-ai</code> <code>alignment</code> <code>safety</code>   Novel approach to AI alignment using constitutional principles and AI feedback.</p> </li> <li> <p>Red Teaming Language Models with Language Models (Perez et al., 2022) EMNLP 2022 | <code>red-teaming</code> <code>llm-safety</code> <code>automated-testing</code>   Automated red teaming approach for identifying harmful LLM behaviors.</p> </li> <li> <p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (Bai et al., 2022) Anthropic | <code>rlhf</code> <code>helpfulness</code> <code>harmlessness</code>   Balancing helpfulness and harmlessness in conversational AI systems.</p> </li> </ul>"},{"location":"research/papers/#benchmark-papers","title":"Benchmark Papers","text":"<ul> <li> <p>BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation (Dhamala et al., 2021) FAccT 2021 | <code>bias-benchmarks</code> <code>language-generation</code>   Comprehensive benchmark for measuring bias in text generation models.</p> </li> <li> <p>RobustBench: a Standardized Adversarial Robustness Benchmark (Croce et al., 2020) NeurIPS 2021 | <code>robustness-benchmark</code> <code>evaluation</code>   Standardized benchmark and leaderboard for adversarial robustness evaluation.</p> </li> </ul>"},{"location":"research/papers/#paper-collections-by-venue","title":"Paper Collections by Venue","text":""},{"location":"research/papers/#top-tier-conferences","title":"Top-Tier Conferences","text":"FAccT (Fairness, Accountability, Transparency)ICML/NeurIPSICLR <ul> <li>FAccT 2024 - Latest fairness and accountability research</li> <li>FAccT 2023 - Includes algorithmic auditing advances</li> <li>FAccT 2022 - Focus on intersectionality and bias</li> </ul> <ul> <li>Focus on theoretical foundations and scalable algorithms</li> <li>Strong representation in robustness and privacy research</li> <li>Recent emphasis on LLM safety and alignment</li> </ul> <ul> <li>Cutting-edge deep learning approaches to trustworthy ML</li> <li>Novel architectures for interpretable models</li> <li>Adversarial robustness innovations</li> </ul>"},{"location":"research/papers/#specialized-venues","title":"Specialized Venues","text":"<ul> <li>AIES (AI, Ethics, and Society): Interdisciplinary perspectives</li> <li>S&amp;P, CCS, USENIX Security: Security and privacy focus  </li> <li>CHI, CSCW: Human-computer interaction and social impacts</li> <li>AAAI: Broad AI applications and theoretical work</li> </ul>"},{"location":"research/papers/#reading-lists-by-course-module","title":"Reading Lists by Course Module","text":""},{"location":"research/papers/#for-assignment-1-bias-detection","title":"For Assignment 1: Bias Detection","text":"<ol> <li>Verma &amp; Rubin (2018) - Fairness definitions overview</li> <li>Bellamy et al. (2019) - Practical fairness toolkit usage</li> <li>Choose one: Group fairness vs. individual fairness comparison</li> </ol>"},{"location":"research/papers/#for-assignment-2-adversarial-robustness","title":"For Assignment 2: Adversarial Robustness","text":"<ol> <li>Goodfellow et al. (2014) - FGSM and basic concepts</li> <li>Madry et al. (2017) - PGD and evaluation methodology</li> <li>Cohen et al. (2019) - Certified defenses introduction</li> </ol>"},{"location":"research/papers/#for-midterm-preparation","title":"For Midterm Preparation","text":"<p>Core papers from each topic area marked with \u2b50 in the full bibliography.</p> <p>Contributing</p> <p>Found an important paper we missed? Submit a suggestion to help keep this library comprehensive and current.</p> <p>Last updated: {{ git_revision_date }}</p>"},{"location":"resources/datasets/","title":"Datasets for Trustworthy ML","text":"<p>A curated collection of datasets commonly used for research and education in trustworthy machine learning.</p>"},{"location":"resources/datasets/#fairness-benchmarks","title":"Fairness Benchmarks","text":""},{"location":"resources/datasets/#traditional-ml-datasets","title":"Traditional ML Datasets","text":"<p>Adult Income Dataset UCI ML Repository | 48K samples | Tabular <pre><code>from aif360.datasets import AdultDataset\ndataset = AdultDataset()\n</code></pre> - Task: Income prediction (&gt;$50K/year) - Protected attributes: Race, gender, age - Use cases: Group fairness, bias detection - Notable papers: Most fairness papers use this dataset</p> <p>COMPAS Recidivism ProPublica | 7K samples | Tabular <pre><code>from aif360.datasets import CompasDataset  \ndataset = CompasDataset()\n</code></pre> - Task: Recidivism risk prediction - Protected attributes: Race, gender, age - Use cases: Criminal justice fairness, algorithmic auditing - Real-world impact: Used in actual court decisions</p>"},{"location":"resources/datasets/#modern-fairness-benchmarks","title":"Modern Fairness Benchmarks","text":"<p>Folktables Stanford | Millions of samples | Census data <pre><code>from folktables import ACSDataSource, ACSIncome\ndata_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n</code></pre> - Tasks: Income, employment, health insurance, travel time - Features: Realistic distribution shifts over time and geography - Best for: Large-scale fairness evaluation, intersectionality</p> <p>CelebA CUHK | 200K images | Face attributes <pre><code>import torchvision.datasets as datasets\ndataset = datasets.CelebA(root='./data', download=True)\n</code></pre> - Task: Multi-label face attribute prediction - Protected attributes: Gender, apparent age, race (inferred) - Use cases: Vision fairness, intersectional bias</p>"},{"location":"resources/datasets/#robustness-benchmarks","title":"Robustness Benchmarks","text":""},{"location":"resources/datasets/#adversarial-robustness","title":"Adversarial Robustness","text":"<p>CIFAR-10/100 University of Toronto | 60K images | Object recognition <pre><code>import torchvision.datasets as datasets\ncifar10 = datasets.CIFAR10(root='./data', download=True)\n</code></pre> - Standard: Most common robustness benchmark - Attacks: FGSM, PGD, C&amp;W, AutoAttack - Defenses: Adversarial training, certified methods</p> <p>ImageNet Stanford | 14M images | Object recognition <pre><code>from torchvision.datasets import ImageNet\ndataset = ImageNet(root='./data', split='val')\n</code></pre> - Scale: Large-scale robustness evaluation - Use cases: Transfer learning robustness, real-world evaluation</p>"},{"location":"resources/datasets/#distribution-shift","title":"Distribution Shift","text":"<p>WILDS Stanford | Multiple domains | Distribution shift <pre><code>from wilds import get_dataset\ndataset = get_dataset(dataset=\"camelyon17\", download=True)\n</code></pre> - Datasets: Medical imaging, wildlife, satellite, text - Shifts: Geographic, temporal, demographic - Evaluation: Worst-group performance, average performance</p> <p>ImageNet-C UC Berkeley | 15 corruption types | Corrupted images <pre><code># Download from authors' website\nimport numpy as np\ncorrupt_data = np.load('imagenet_c/gaussian_noise/5/') \n</code></pre> - Corruptions: Weather, blur, noise, digital artifacts - Severity: 5 levels of corruption intensity - Use cases: Natural robustness evaluation</p>"},{"location":"resources/datasets/#privacy-benchmarks","title":"Privacy Benchmarks","text":""},{"location":"resources/datasets/#federated-learning","title":"Federated Learning","text":"<p>LEAF CMU | Multiple tasks | Federated setting <pre><code># Use LEAF data loaders\nfrom leaf.data_utils import read_data\nclients, groups, data = read_data('femnist')\n</code></pre> - Datasets: FEMNIST, CelebA, Reddit, Shakespeare - Properties: Non-IID data distribution, realistic client heterogeneity - Use cases: Federated learning algorithms, privacy evaluation</p> <p>FLamby Inria | Medical data | Cross-silo FL <pre><code>from flamby.datasets.fed_heart_disease import FedHeartDisease\ndataset = FedHeartDisease(center=0, train=True)\n</code></pre> - Focus: Medical federated learning - Datasets: Heart disease, skin cancer, drug discovery - Realistic: Based on real medical collaborations</p>"},{"location":"resources/datasets/#differential-privacy","title":"Differential Privacy","text":"<p>Adult + DP Mechanisms Google | Various | DP training examples <pre><code>from tensorflow_privacy.privacy.optimizers import dp_optimizer\noptimizer = dp_optimizer.DPGradientDescentGaussianOptimizer(\n    l2_norm_clip=1.0, noise_multiplier=1.1, learning_rate=0.01)\n</code></pre> - Benchmarks: Standard datasets with DP training - Metrics: Privacy budget vs. accuracy trade-offs</p>"},{"location":"resources/datasets/#interpretability-datasets","title":"Interpretability Datasets","text":""},{"location":"resources/datasets/#feature-attribution","title":"Feature Attribution","text":"<p>Boston Housing UCI | 506 samples | Regression <pre><code>from sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)  # Note: deprecated due to ethical concerns\n</code></pre> - Use cases: Feature importance, explanation evaluation - Note: Consider alternatives due to ethical concerns</p> <p>Wine Quality UCI | 6K samples | Regression/Classification <pre><code>import pandas as pd\ndata = pd.read_csv('winequality-red.csv', delimiter=';')\n</code></pre> - Features: Chemical properties affecting wine quality - Use cases: Feature attribution, counterfactual explanations</p>"},{"location":"resources/datasets/#computer-vision","title":"Computer Vision","text":"<p>ImageNet + Attribution Google | 14M images | Saliency evaluation <pre><code>import saliency.core as saliency\ngradient_saliency = saliency.GradientSaliency()\n</code></pre> - Methods: GradCAM, Integrated Gradients, LIME - Evaluation: Pointing game, deletion/insertion metrics</p>"},{"location":"resources/datasets/#specialized-domains","title":"Specialized Domains","text":""},{"location":"resources/datasets/#natural-language-processing","title":"Natural Language Processing","text":"<p>BOLD Amazon | Text generation | Bias evaluation <pre><code>from bold import BoldDataset\ndataset = BoldDataset()\n</code></pre> - Focus: Bias in open-ended text generation - Attributes: Gender, race, religion, political ideology - Metrics: Sentiment, toxicity, regard</p> <p>Winogender Johns Hopkins | Coreference | Gender bias <pre><code>import json\nwith open('data/winogender-schemas.txt') as f:\n    templates = f.readlines()\n</code></pre> - Task: Pronoun coreference resolution - Bias: Gender stereotypes in occupations - Use cases: NLP fairness evaluation</p>"},{"location":"resources/datasets/#healthcare","title":"Healthcare","text":"<p>MIMIC-III MIT | 40K patients | Clinical records <pre><code># Requires credentialed access\nimport pandas as pd\nadmissions = pd.read_csv('ADMISSIONS.csv')\n</code></pre> - Access: Requires training and approval - Use cases: Healthcare fairness, privacy-preserving ML - Protected attributes: Race, gender, insurance</p>"},{"location":"resources/datasets/#finance","title":"Finance","text":"<p>German Credit UCI | 1K samples | Credit risk <pre><code>from aif360.datasets import GermanDataset\ndataset = GermanDataset()\n</code></pre> - Task: Credit risk assessment - Protected attributes: Age, gender, foreign worker status - Use cases: Financial fairness, regulatory compliance</p>"},{"location":"resources/datasets/#dataset-usage-guidelines","title":"Dataset Usage Guidelines","text":""},{"location":"resources/datasets/#fairness-analysis","title":"Fairness Analysis","text":"<ol> <li>Identify protected attributes in the dataset</li> <li>Define fairness metrics appropriate for the task  </li> <li>Evaluate intersectional effects across multiple attributes</li> <li>Consider historical bias in data collection</li> </ol>"},{"location":"resources/datasets/#robustness-testing","title":"Robustness Testing","text":"<ol> <li>Start with clean accuracy as baseline</li> <li>Apply systematic attacks with increasing strength</li> <li>Test multiple threat models (white-box, black-box)</li> <li>Evaluate on distribution shifts relevant to deployment</li> </ol>"},{"location":"resources/datasets/#privacy-evaluation","title":"Privacy Evaluation","text":"<ol> <li>Implement membership inference attacks as baseline</li> <li>Measure privacy-utility trade-offs across \u03b5 values</li> <li>Test reconstruction attacks where applicable</li> <li>Validate privacy guarantees with formal analysis</li> </ol>"},{"location":"resources/datasets/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"resources/datasets/#data-usage","title":"Data Usage","text":"<ul> <li>Consent: Ensure appropriate consent for research use</li> <li>Bias: Acknowledge limitations and potential biases</li> <li>Privacy: Follow data protection regulations (GDPR, etc.)</li> <li>Attribution: Cite original data sources appropriately</li> </ul>"},{"location":"resources/datasets/#sensitive-attributes","title":"Sensitive Attributes","text":"<ul> <li>Protected characteristics: Handle race, gender, etc. with care</li> <li>Intersectionality: Consider multiple overlapping identities</li> <li>Historical context: Understand societal biases in data</li> <li>Representation: Ensure diverse and inclusive datasets</li> </ul> <p>Dataset Deprecations</p> <p>Some datasets (e.g., Boston Housing) have been deprecated due to ethical concerns. Always check for recommended alternatives and consider the ethical implications of your dataset choices.</p> <p>Last updated: December 2024</p>"},{"location":"resources/tools/","title":"Tools &amp; Frameworks","text":"<p>A comprehensive collection of open-source tools, libraries, and frameworks for implementing trustworthy machine learning systems.</p>"},{"location":"resources/tools/#fairness-bias-mitigation","title":"Fairness &amp; Bias Mitigation","text":""},{"location":"resources/tools/#comprehensive-toolkits","title":"Comprehensive Toolkits","text":"<p>AI Fairness 360 (AIF360) IBM Research | Python, R | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install aif360\n</code></pre> - Features: 70+ fairness metrics, 10+ bias mitigation algorithms - Best for: Research, comprehensive bias analysis, enterprise applications - Highlights: Web interface, extensive documentation, industry-tested - Example: Credit scoring, hiring decisions, criminal justice</p> <p>Fairlearn Microsoft | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install fairlearn\n</code></pre> - Features: Scikit-learn integration, dashboard visualization, constraint-based optimization - Best for: Quick prototyping, ML practitioners familiar with sklearn - Highlights: User-friendly API, interactive dashboards, Azure ML integration</p>"},{"location":"resources/tools/#specialized-libraries","title":"Specialized Libraries","text":"<p>Themis UMass Amherst | Python | \u2b50\u2b50\u2b50 <pre><code>pip install themis-ml\n</code></pre> - Focus: Fairness testing and debugging - Features: Automated bias discovery, causal fairness analysis - Best for: Testing existing models for hidden biases</p> <p>FairML Academic | Python | \u2b50\u2b50\u2b50 <pre><code>pip install fairml\n</code></pre> - Focus: Auditing black-box models - Features: Input influence ranking, bias detection without model access - Best for: Third-party model auditing</p>"},{"location":"resources/tools/#robustness-adversarial-defense","title":"Robustness &amp; Adversarial Defense","text":""},{"location":"resources/tools/#attack-libraries","title":"Attack Libraries","text":"<p>Adversarial Robustness Toolbox (ART) IBM Research | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install adversarial-robustness-toolbox\n</code></pre> - Features: 20+ attacks, 15+ defenses, multiple ML frameworks - Frameworks: TensorFlow, PyTorch, scikit-learn, XGBoost - Best for: Comprehensive adversarial ML research and testing</p> <p>Foolbox University of T\u00fcbingen | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install foolbox\n</code></pre> - Features: 30+ gradient-based and black-box attacks - Frameworks: PyTorch, TensorFlow, JAX, NumPy - Best for: Quick adversarial example generation, benchmarking</p> <p>CleverHans Google Brain | Python | \u2b50\u2b50\u2b50 <pre><code>pip install cleverhans\n</code></pre> - Features: Classic attacks (FGSM, PGD, C&amp;W), TensorFlow focus - Best for: Educational purposes, reproducing classic papers</p>"},{"location":"resources/tools/#defense-frameworks","title":"Defense Frameworks","text":"<p>CROWN UCLA | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>git clone https://github.com/huanzhang12/CROWN-IBP\n</code></pre> - Focus: Certified robustness via interval bound propagation - Features: Formal verification, scalable certified training - Best for: Safety-critical applications requiring guarantees</p> <p>Auto-Attack EPFL | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install autoattack\n</code></pre> - Focus: Robust evaluation standard - Features: Ensemble of complementary attacks, parameter-free - Best for: Standardized robustness evaluation</p>"},{"location":"resources/tools/#interpretability-explainability","title":"Interpretability &amp; Explainability","text":""},{"location":"resources/tools/#model-agnostic-tools","title":"Model-Agnostic Tools","text":"<p>SHAP Microsoft Research | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install shap\n</code></pre> - Theory: Shapley values from cooperative game theory - Features: Global/local explanations, 15+ explainer types - Visualization: Interactive plots, force plots, dependence plots - Best for: Production explanations, business stakeholders</p> <p>LIME University of Washington | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install lime\n</code></pre> - Theory: Local linear approximation - Features: Text, image, tabular data support - Best for: Quick local explanations, diverse data types</p>"},{"location":"resources/tools/#deep-learning-specific","title":"Deep Learning Specific","text":"<p>Captum PyTorch Team | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install captum\n</code></pre> - Framework: Native PyTorch integration - Features: 15+ attribution algorithms, neuron/layer analysis - Visualization: Built-in visualization utilities - Best for: Deep learning research, PyTorch users</p> <p>Alibi Seldon | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install alibi\n</code></pre> - Features: Counterfactual explanations, anchor explanations - Focus: Production-ready explanations for ML deployment - Best for: Model serving, real-time explanations</p> <p>InterpretML Microsoft Research | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install interpret\n</code></pre> - Features: Glass-box models (EBM), model-agnostic explanations - Visualization: Unified dashboard for multiple explanation types - Best for: Regulated industries, healthcare, finance</p>"},{"location":"resources/tools/#privacy-preserving-ml","title":"Privacy-Preserving ML","text":""},{"location":"resources/tools/#differential-privacy","title":"Differential Privacy","text":"<p>Opacus PyTorch Team | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install opacus\n</code></pre> - Framework: PyTorch-native differential privacy - Features: DP-SGD, privacy accounting, gradient clipping - Best for: Deep learning with formal privacy guarantees</p> <p>TensorFlow Privacy Google | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install tensorflow-privacy\n</code></pre> - Framework: TensorFlow integration - Features: DP optimizers, privacy analysis, membership inference - Best for: Large-scale DP training, Google Cloud integration</p> <p>Diffprivlib IBM Research | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install diffprivlib\n</code></pre> - Framework: Scikit-learn compatible DP algorithms - Features: DP versions of common ML algorithms - Best for: Traditional ML with differential privacy</p>"},{"location":"resources/tools/#federated-learning","title":"Federated Learning","text":"<p>PySyft OpenMined | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install syft\n</code></pre> - Features: Federated learning, secure multi-party computation - Frameworks: PyTorch, TensorFlow support - Best for: Research, privacy-preserving collaborations</p> <p>Flower (flwr) Flower Labs | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install flwr\n</code></pre> - Features: Framework-agnostic federated learning - Deployment: Easy client-server architecture - Best for: Production federated learning, cross-platform deployment</p> <p>FedML FedML Inc | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install fedml\n</code></pre> - Features: MLOps for federated learning, mobile deployment - Platform: Cloud platform + open source library - Best for: End-to-end federated ML solutions</p>"},{"location":"resources/tools/#evaluation-benchmarking","title":"Evaluation &amp; Benchmarking","text":""},{"location":"resources/tools/#robustness-benchmarks","title":"Robustness Benchmarks","text":"<p>RobustBench Community | Python | \u2b50\u2b50\u2b50\u2b50\u2b50 <pre><code>pip install robustbench\n</code></pre> - Features: Standardized robustness evaluation, model zoo - Datasets: CIFAR-10/100, ImageNet, common corruptions - Best for: Comparing robustness methods, reproducible evaluation</p>"},{"location":"resources/tools/#fairness-benchmarks","title":"Fairness Benchmarks","text":"<p>Folktables Stanford | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install folktables\n</code></pre> - Features: Real-world fairness benchmarks from US Census data - Tasks: Income prediction, employment, health insurance - Best for: Realistic fairness evaluation, policy research</p>"},{"location":"resources/tools/#development-deployment","title":"Development &amp; Deployment","text":""},{"location":"resources/tools/#mlops-for-trustworthy-ml","title":"MLOps for Trustworthy ML","text":"<p>Evidently Evidently AI | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install evidently\n</code></pre> - Features: ML monitoring, drift detection, bias monitoring - Deployment: Dashboard, reports, real-time monitoring - Best for: Production ML monitoring, continuous auditing</p> <p>Great Expectations Superconductive | Python | \u2b50\u2b50\u2b50\u2b50 <pre><code>pip install great-expectations\n</code></pre> - Features: Data validation, pipeline testing, documentation - Integration: Airflow, dbt, cloud platforms - Best for: Data quality assurance, ML pipeline validation</p>"},{"location":"resources/tools/#model-cards-documentation","title":"Model Cards &amp; Documentation","text":"<p>Model Card Toolkit Google | Python | \u2b50\u2b50\u2b50 <pre><code>pip install model-card-toolkit\n</code></pre> - Features: Automated model card generation, templates - Integration: TensorFlow Model Analysis integration - Best for: Model documentation, regulatory compliance</p>"},{"location":"resources/tools/#quick-start-guides","title":"Quick Start Guides","text":""},{"location":"resources/tools/#fairness-analysis-workflow","title":"Fairness Analysis Workflow","text":"<pre><code># Using AIF360 for comprehensive bias analysis\nfrom aif360.datasets import AdultDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.algorithms.preprocessing import Reweighing\n\n# Load data and compute bias metrics\ndataset = AdultDataset()\nmetric = BinaryLabelDatasetMetric(dataset)\nprint(f\"Disparate Impact: {metric.disparate_impact()}\")\n\n# Apply bias mitigation\nrw = Reweighing(unprivileged_groups=[{'sex': 0}],\n                privileged_groups=[{'sex': 1}])\ndataset_transf = rw.fit_transform(dataset)\n</code></pre>"},{"location":"resources/tools/#adversarial-robustness-testing","title":"Adversarial Robustness Testing","text":"<pre><code># Using ART for adversarial evaluation\nfrom art.attacks.evasion import FastGradientMethod\nfrom art.estimators.classification import KerasClassifier\n\n# Wrap your model\nclassifier = KerasClassifier(model=model)\n\n# Generate adversarial examples\nattack = FastGradientMethod(estimator=classifier, eps=0.1)\nx_test_adv = attack.generate(x=x_test)\n\n# Evaluate robustness\naccuracy_clean = classifier.predict(x_test).argmax(axis=1) == y_test\naccuracy_adv = classifier.predict(x_test_adv).argmax(axis=1) == y_test\nprint(f\"Clean accuracy: {accuracy_clean.mean():.2f}\")\nprint(f\"Adversarial accuracy: {accuracy_adv.mean():.2f}\")\n</code></pre>"},{"location":"resources/tools/#privacy-preserving-training","title":"Privacy-Preserving Training","text":"<pre><code># Using Opacus for differential privacy\nfrom opacus import PrivacyEngine\n\n# Attach privacy engine to optimizer\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, data_loader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=data_loader,\n    noise_multiplier=1.0,\n    max_grad_norm=1.0,\n)\n\n# Train with privacy guarantees\nfor epoch in range(epochs):\n    for batch in data_loader:\n        # Standard PyTorch training loop\n        optimizer.zero_grad()\n        loss = criterion(model(batch[0]), batch[1])\n        loss.backward()\n        optimizer.step()\n\n    # Check privacy budget\n    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n    print(f\"Epoch {epoch}, \u03b5 = {epsilon:.2f}\")\n</code></pre> <p>Tool Selection Guide</p> <ul> <li>Research: Start with comprehensive toolkits (AIF360, ART, SHAP)</li> <li>Production: Focus on framework-specific tools (Fairlearn for sklearn, Captum for PyTorch)</li> <li>Evaluation: Use standardized benchmarks (RobustBench, Folktables)</li> <li>Deployment: Implement monitoring (Evidently, Great Expectations)</li> </ul> <p>Last updated: December 2024</p>"}]}